{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccd5d83c-eef4-4bb8-8153-7357e49aa403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FINAL FIXED FAKE NEWS DETECTION SYSTEM ===\n",
      "\n",
      "Creating expanded dataset...\n",
      "Dataset created: 30 articles\n",
      "Real news: 15, Fake news: 15\n",
      "Preprocessing and extracting features...\n",
      "Linguistic features shape: (30, 14)\n",
      "Linguistic features min: 0.0\n",
      "Linguistic features max: 117.0\n",
      "TF-IDF features shape: (30, 439)\n",
      "TF-IDF features min: 0.0\n",
      "TF-IDF features max: 0.42921607226022546\n",
      "Scaled linguistic features min: 0.0\n",
      "Scaled linguistic features max: 1.0\n",
      "Combined features shape: (30, 453)\n",
      "Combined features min: 0.0\n",
      "Combined features max: 1.0\n",
      "\n",
      "Data split completed:\n",
      "Training set: 21 samples\n",
      "Test set: 9 samples\n",
      "Training labels: Fake=10, Real=11\n",
      "Test labels: Fake=5, Real=4\n",
      "Training models...\n",
      "\n",
      "Training Naive Bayes...\n",
      "Naive Bayes training accuracy: 1.000\n",
      "Naive Bayes training successful!\n",
      "\n",
      "Training Logistic Regression...\n",
      "Logistic Regression training accuracy: 1.000\n",
      "Logistic Regression training successful!\n",
      "\n",
      "Training Random Forest...\n",
      "Random Forest training accuracy: 1.000\n",
      "Random Forest training successful!\n",
      "\n",
      "Model training completed! Successfully trained 3 models.\n",
      "\n",
      "Evaluating models on test set...\n",
      "\n",
      "Naive Bayes Results:\n",
      "Accuracy: 1.0000\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "           1       1.00      1.00      1.00         4\n",
      "\n",
      "    accuracy                           1.00         9\n",
      "   macro avg       1.00      1.00      1.00         9\n",
      "weighted avg       1.00      1.00      1.00         9\n",
      "\n",
      "Confusion Matrix:\n",
      "[[5 0]\n",
      " [0 4]]\n",
      "\n",
      "Logistic Regression Results:\n",
      "Accuracy: 1.0000\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "           1       1.00      1.00      1.00         4\n",
      "\n",
      "    accuracy                           1.00         9\n",
      "   macro avg       1.00      1.00      1.00         9\n",
      "weighted avg       1.00      1.00      1.00         9\n",
      "\n",
      "Confusion Matrix:\n",
      "[[5 0]\n",
      " [0 4]]\n",
      "\n",
      "Random Forest Results:\n",
      "Accuracy: 1.0000\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "           1       1.00      1.00      1.00         4\n",
      "\n",
      "    accuracy                           1.00         9\n",
      "   macro avg       1.00      1.00      1.00         9\n",
      "weighted avg       1.00      1.00      1.00         9\n",
      "\n",
      "Confusion Matrix:\n",
      "[[5 0]\n",
      " [0 4]]\n",
      "\n",
      "============================================================\n",
      "TESTING WITH SAMPLE ARTICLES\n",
      "============================================================\n",
      "\n",
      "Test Article 1: Scientists at MIT have developed a revolutionary new battery technology accordin...\n",
      "Preprocessing and extracting features...\n",
      "Linguistic features shape: (1, 14)\n",
      "Linguistic features min: 0.0\n",
      "Linguistic features max: 136.0\n",
      "TF-IDF features shape: (1, 439)\n",
      "TF-IDF features min: 0.0\n",
      "TF-IDF features max: 0.2637415503288022\n",
      "Scaled linguistic features min: 0.0\n",
      "Scaled linguistic features max: 1.375\n",
      "Combined features shape: (1, 453)\n",
      "Combined features min: 0.0\n",
      "Combined features max: 1.375\n",
      "Naive Bayes: Real (Confidence: 0.999)\n",
      "Logistic Regression: Real (Confidence: 0.964)\n",
      "Random Forest: Real (Confidence: 0.980)\n",
      "Ensemble: Real (Confidence: 1.000)\n",
      "\n",
      "Test Article 2: BREAKING: Government officials confirm alien contact, world leaders to make SHOC...\n",
      "Preprocessing and extracting features...\n",
      "Linguistic features shape: (1, 14)\n",
      "Linguistic features min: 0.0\n",
      "Linguistic features max: 109.0\n",
      "TF-IDF features shape: (1, 439)\n",
      "TF-IDF features min: 0.0\n",
      "TF-IDF features max: 0.4022628262909764\n",
      "Scaled linguistic features min: -0.0625\n",
      "Scaled linguistic features max: 3.0\n",
      "Combined features shape: (1, 453)\n",
      "Combined features min: -0.0625\n",
      "Combined features max: 3.0\n",
      "WARNING: Negative values detected, applying absolute transformation\n",
      "Naive Bayes: Fake (Confidence: 1.000)\n",
      "Logistic Regression: Fake (Confidence: 0.980)\n",
      "Random Forest: Fake (Confidence: 0.640)\n",
      "Ensemble: Fake (Confidence: 1.000)\n",
      "\n",
      "Test Article 3: Local hospital reports successful treatment outcomes with new COVID-19 therapy i...\n",
      "Preprocessing and extracting features...\n",
      "Linguistic features shape: (1, 14)\n",
      "Linguistic features min: 0.0\n",
      "Linguistic features max: 108.0\n",
      "TF-IDF features shape: (1, 439)\n",
      "TF-IDF features min: 0.0\n",
      "TF-IDF features max: 0.28577332940656847\n",
      "Scaled linguistic features min: 0.0\n",
      "Scaled linguistic features max: 0.875\n",
      "Combined features shape: (1, 453)\n",
      "Combined features min: 0.0\n",
      "Combined features max: 0.875\n",
      "Naive Bayes: Real (Confidence: 0.980)\n",
      "Logistic Regression: Real (Confidence: 0.888)\n",
      "Random Forest: Real (Confidence: 0.900)\n",
      "Ensemble: Real (Confidence: 1.000)\n",
      "Models saved to final_fake_news_detector.pkl\n",
      "\n",
      "============================================================\n",
      "SYSTEM TRAINING AND TESTING COMPLETE!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Final Fixed Fake News Detection System\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "class FinalFakeNewsDetector:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the Fixed Fake News Detection System\"\"\"\n",
    "        self.tfidf_vectorizer = None\n",
    "        self.scaler = None\n",
    "        self.models = {}\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.feature_names = []\n",
    "        \n",
    "    def create_expanded_dataset(self):\n",
    "        \"\"\"Create a larger, balanced dataset for better training\"\"\"\n",
    "        \n",
    "        # Fake news examples with clear patterns\n",
    "        fake_news = [\n",
    "            \"BREAKING: Scientists HATE this one weird trick that cures cancer instantly!\",\n",
    "            \"SHOCKING revelation: Government hiding alien technology for decades!\",\n",
    "            \"You won't BELIEVE what doctors found in this man's stomach!\",\n",
    "            \"URGENT: New study proves vaccines contain mind control chips!\",\n",
    "            \"INCREDIBLE: Local mom discovers miracle weight loss secret!\",\n",
    "            \"AMAZING: This simple trick will make you rich overnight!\",\n",
    "            \"UNBELIEVABLE: Child predicts future with 100% accuracy!\",\n",
    "            \"SECRET government plan to control weather revealed by whistleblower!\",\n",
    "            \"MIRACLE cure found in grandmother's attic eliminates all diseases!\",\n",
    "            \"TERRIFYING: 5G towers causing mass bird deaths worldwide!\",\n",
    "            \"EXCLUSIVE: Celebrity admits to being controlled by illuminati!\",\n",
    "            \"EXPOSED: Hospitals hiding cure for diabetes to make profits!\",\n",
    "            \"STUNNING: Man lives 200 years using this one simple method!\",\n",
    "            \"OUTRAGEOUS: Schools teaching children to worship Satan!\",\n",
    "            \"BOMBSHELL: Water fluoridation linked to zombie outbreak!\"\n",
    "        ]\n",
    "        \n",
    "        # Real news examples with credible patterns\n",
    "        real_news = [\n",
    "            \"Researchers at Stanford University published findings on renewable energy efficiency in peer-reviewed journal Nature.\",\n",
    "            \"The Federal Reserve announced a 0.25% interest rate increase following comprehensive economic data analysis.\",\n",
    "            \"Local hospital reports 15% decrease in COVID-19 cases over two-week period according to health officials.\",\n",
    "            \"NASA's James Webb telescope captures detailed images of distant galaxy formation in unprecedented detail.\",\n",
    "            \"City council approves $2.3 million budget for infrastructure improvements and public services enhancement.\",\n",
    "            \"University study shows meditation may reduce stress levels in clinical trial with 200 participants.\",\n",
    "            \"Environmental Protection Agency releases new guidelines for air quality monitoring in urban areas.\",\n",
    "            \"Medical researchers report promising results in early-stage cancer treatment trials at Johns Hopkins.\",\n",
    "            \"Economic indicators suggest moderate growth in manufacturing sector this quarter, analysts report.\",\n",
    "            \"Scientists at MIT develop new battery technology for electric vehicle applications in laboratory setting.\",\n",
    "            \"Health officials recommend updated vaccination schedules based on current epidemiological data.\",\n",
    "            \"Archaeological team discovers 3,000-year-old artifacts in systematic excavation near ancient city.\",\n",
    "            \"Climate research team publishes study on Arctic ice patterns in Journal of Climate Science.\",\n",
    "            \"University hospital announces successful organ transplant program expansion with improved outcomes.\",\n",
    "            \"Technology conference showcases advances in artificial intelligence applications for medical diagnosis.\"\n",
    "        ]\n",
    "        \n",
    "        # Create balanced dataset\n",
    "        texts = fake_news + real_news\n",
    "        labels = [0] * len(fake_news) + [1] * len(real_news)  # 0 = Fake, 1 = Real\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'text': texts,\n",
    "            'label': labels\n",
    "        })\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Enhanced text preprocessing\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "            \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove URLs, emails, and special patterns\n",
    "        text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        \n",
    "        # Remove special characters but keep basic punctuation\n",
    "        text = re.sub(r'[^\\w\\s!?.,]', '', text)\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Remove stopwords and stem (preserve some indicators)\n",
    "        processed_tokens = []\n",
    "        for token in tokens:\n",
    "            if token not in self.stop_words or token in ['!', '?']:\n",
    "                if token.isalpha():\n",
    "                    processed_tokens.append(self.stemmer.stem(token))\n",
    "                else:\n",
    "                    processed_tokens.append(token)\n",
    "        \n",
    "        return ' '.join(processed_tokens)\n",
    "    \n",
    "    def extract_linguistic_features(self, text):\n",
    "        \"\"\"Extract comprehensive linguistic features\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        if not text or len(text.strip()) == 0:\n",
    "            # Return zero features for empty text\n",
    "            feature_names = [\n",
    "                'char_count', 'word_count', 'sentence_count', 'avg_word_length',\n",
    "                'avg_sentence_length', 'exclamation_count', 'question_count',\n",
    "                'capital_ratio', 'exclamation_ratio', 'emotional_word_count',\n",
    "                'emotional_word_ratio', 'credibility_indicators', 'clickbait_count', 'caps_word_count'\n",
    "            ]\n",
    "            return {name: 0.0 for name in feature_names}\n",
    "        \n",
    "        # Basic text statistics\n",
    "        words = text.split()\n",
    "        sentences = sent_tokenize(text)\n",
    "        \n",
    "        features['char_count'] = float(len(text))\n",
    "        features['word_count'] = float(len(words))\n",
    "        features['sentence_count'] = float(max(len(sentences), 1))\n",
    "        features['avg_word_length'] = float(np.mean([len(word) for word in words]) if words else 0)\n",
    "        features['avg_sentence_length'] = float(len(words) / max(len(sentences), 1))\n",
    "        \n",
    "        # Punctuation and style features\n",
    "        features['exclamation_count'] = float(text.count('!'))\n",
    "        features['question_count'] = float(text.count('?'))\n",
    "        features['capital_ratio'] = float(sum(1 for c in text if c.isupper()) / max(len(text), 1))\n",
    "        features['exclamation_ratio'] = float(features['exclamation_count'] / max(len(sentences), 1))\n",
    "        \n",
    "        # Emotional and sensational language\n",
    "        emotional_words = [\n",
    "            'amazing', 'shocking', 'unbelievable', 'incredible', 'outrageous', \n",
    "            'stunning', 'terrifying', 'miracle', 'secret', 'exposed', 'breaking',\n",
    "            'urgent', 'exclusive', 'revealed', 'discovered', 'bombshell'\n",
    "        ]\n",
    "        emotional_count = sum(1 for word in emotional_words if word in text.lower())\n",
    "        features['emotional_word_count'] = float(emotional_count)\n",
    "        features['emotional_word_ratio'] = float(emotional_count / max(len(words), 1))\n",
    "        \n",
    "        # Credibility indicators\n",
    "        credible_phrases = [\n",
    "            'according to', 'study shows', 'research indicates', 'scientists found',\n",
    "            'university', 'published', 'peer-reviewed', 'clinical trial', 'data suggests'\n",
    "        ]\n",
    "        features['credibility_indicators'] = float(sum(1 for phrase in credible_phrases if phrase in text.lower()))\n",
    "        \n",
    "        # Clickbait indicators\n",
    "        clickbait_words = ['you won\\'t believe', 'doctors hate', 'one weird trick', 'this will shock']\n",
    "        features['clickbait_count'] = float(sum(1 for phrase in clickbait_words if phrase in text.lower()))\n",
    "        \n",
    "        # All caps words (shouting)\n",
    "        features['caps_word_count'] = float(sum(1 for word in words if word.isupper() and len(word) > 1))\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def prepare_features(self, df):\n",
    "        \"\"\"Prepare features ensuring non-negative values\"\"\"\n",
    "        print(\"Preprocessing and extracting features...\")\n",
    "        \n",
    "        # Preprocess text\n",
    "        df['cleaned_text'] = df['text'].apply(self.preprocess_text)\n",
    "        \n",
    "        # Extract linguistic features\n",
    "        linguistic_features = []\n",
    "        for text in df['text']:\n",
    "            features = self.extract_linguistic_features(text)\n",
    "            linguistic_features.append(features)\n",
    "        \n",
    "        linguistic_df = pd.DataFrame(linguistic_features)\n",
    "        self.feature_names = list(linguistic_df.columns)\n",
    "        \n",
    "        # Handle any NaN values\n",
    "        linguistic_df = linguistic_df.fillna(0.0)\n",
    "        \n",
    "        # Ensure all linguistic features are non-negative (they should be by design)\n",
    "        linguistic_df = linguistic_df.abs()\n",
    "        \n",
    "        print(f\"Linguistic features shape: {linguistic_df.shape}\")\n",
    "        print(f\"Linguistic features min: {linguistic_df.min().min()}\")\n",
    "        print(f\"Linguistic features max: {linguistic_df.max().max()}\")\n",
    "        \n",
    "        # TF-IDF vectorization\n",
    "        if self.tfidf_vectorizer is None:\n",
    "            self.tfidf_vectorizer = TfidfVectorizer(\n",
    "                max_features=500,  # Reduced for stability\n",
    "                ngram_range=(1, 2),\n",
    "                min_df=1,\n",
    "                max_df=0.95,\n",
    "                stop_words='english'\n",
    "            )\n",
    "            tfidf_features = self.tfidf_vectorizer.fit_transform(df['cleaned_text'])\n",
    "        else:\n",
    "            tfidf_features = self.tfidf_vectorizer.transform(df['cleaned_text'])\n",
    "        \n",
    "        tfidf_dense = tfidf_features.toarray()\n",
    "        print(f\"TF-IDF features shape: {tfidf_dense.shape}\")\n",
    "        print(f\"TF-IDF features min: {tfidf_dense.min()}\")\n",
    "        print(f\"TF-IDF features max: {tfidf_dense.max()}\")\n",
    "        \n",
    "        # Scale linguistic features to [0,1] range\n",
    "        if self.scaler is None:\n",
    "            self.scaler = MinMaxScaler()\n",
    "            linguistic_scaled = self.scaler.fit_transform(linguistic_df.values)\n",
    "        else:\n",
    "            linguistic_scaled = self.scaler.transform(linguistic_df.values)\n",
    "        \n",
    "        print(f\"Scaled linguistic features min: {linguistic_scaled.min()}\")\n",
    "        print(f\"Scaled linguistic features max: {linguistic_scaled.max()}\")\n",
    "        \n",
    "        # Combine features (both are now guaranteed to be non-negative)\n",
    "        combined_features = np.hstack([tfidf_dense, linguistic_scaled])\n",
    "        \n",
    "        print(f\"Combined features shape: {combined_features.shape}\")\n",
    "        print(f\"Combined features min: {combined_features.min()}\")\n",
    "        print(f\"Combined features max: {combined_features.max()}\")\n",
    "        \n",
    "        # Final check for negative values\n",
    "        if combined_features.min() < 0:\n",
    "            print(\"WARNING: Negative values detected, applying absolute transformation\")\n",
    "            combined_features = np.abs(combined_features)\n",
    "        \n",
    "        return combined_features, df['label'].values\n",
    "    \n",
    "    def train_models(self, X_train, y_train):\n",
    "        \"\"\"Train models with error handling\"\"\"\n",
    "        print(\"Training models...\")\n",
    "        \n",
    "        # Models optimized for the dataset\n",
    "        models = {\n",
    "            'Naive Bayes': MultinomialNB(alpha=1.0),\n",
    "            'Logistic Regression': LogisticRegression(\n",
    "                random_state=42, \n",
    "                C=1.0,\n",
    "                max_iter=1000,\n",
    "                solver='liblinear'\n",
    "            ),\n",
    "            'Random Forest': RandomForestClassifier(\n",
    "                n_estimators=50,\n",
    "                random_state=42,\n",
    "                max_depth=10,\n",
    "                min_samples_split=2,\n",
    "                min_samples_leaf=1\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # Train models with error handling\n",
    "        for name, model in models.items():\n",
    "            print(f\"\\nTraining {name}...\")\n",
    "            \n",
    "            try:\n",
    "                # Train model\n",
    "                model.fit(X_train, y_train)\n",
    "                self.models[name] = model\n",
    "                \n",
    "                # Quick accuracy check on training data\n",
    "                train_pred = model.predict(X_train)\n",
    "                train_acc = accuracy_score(y_train, train_pred)\n",
    "                print(f\"{name} training accuracy: {train_acc:.3f}\")\n",
    "                print(f\"{name} training successful!\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error training {name}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "        print(f\"\\nModel training completed! Successfully trained {len(self.models)} models.\")\n",
    "        if len(self.models) == 0:\n",
    "            print(\"ERROR: No models were successfully trained!\")\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def evaluate_models(self, X_test, y_test):\n",
    "        \"\"\"Evaluate model performance\"\"\"\n",
    "        print(\"\\nEvaluating models on test set...\")\n",
    "        \n",
    "        if not self.models:\n",
    "            print(\"No models available for evaluation!\")\n",
    "            return {}\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            try:\n",
    "                # Make predictions\n",
    "                y_pred = model.predict(X_test)\n",
    "                y_pred_proba = model.predict_proba(X_test)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                \n",
    "                results[name] = {\n",
    "                    'accuracy': accuracy,\n",
    "                    'predictions': y_pred,\n",
    "                    'probabilities': y_pred_proba,\n",
    "                    'classification_report': classification_report(y_test, y_pred, zero_division=0)\n",
    "                }\n",
    "                \n",
    "                print(f\"\\n{name} Results:\")\n",
    "                print(f\"Accuracy: {accuracy:.4f}\")\n",
    "                print(\"Classification Report:\")\n",
    "                print(classification_report(y_test, y_pred, zero_division=0))\n",
    "                \n",
    "                # Confusion Matrix\n",
    "                cm = confusion_matrix(y_test, y_pred)\n",
    "                print(f\"Confusion Matrix:\\n{cm}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating {name}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def predict_single_article(self, text):\n",
    "        \"\"\"Make predictions on a single article\"\"\"\n",
    "        if not self.models:\n",
    "            return {\"error\": \"No models available for prediction\"}\n",
    "        \n",
    "        try:\n",
    "            # Create temporary dataframe for preprocessing\n",
    "            temp_df = pd.DataFrame({'text': [text], 'label': [0]})  # Dummy label\n",
    "            \n",
    "            # Prepare features\n",
    "            X, _ = self.prepare_features(temp_df)\n",
    "            \n",
    "            # Get predictions from all models\n",
    "            predictions = {}\n",
    "            votes = {'Fake': 0, 'Real': 0}\n",
    "            \n",
    "            for name, model in self.models.items():\n",
    "                try:\n",
    "                    pred = model.predict(X)[0]\n",
    "                    proba = model.predict_proba(X)[0]\n",
    "                    \n",
    "                    prediction_label = 'Real' if pred == 1 else 'Fake'\n",
    "                    confidence = float(max(proba))\n",
    "                    \n",
    "                    predictions[name] = {\n",
    "                        'prediction': prediction_label,\n",
    "                        'confidence': confidence\n",
    "                    }\n",
    "                    \n",
    "                    votes[prediction_label] += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error in prediction with {name}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            # Ensemble prediction\n",
    "            if votes['Fake'] + votes['Real'] > 0:\n",
    "                ensemble_prediction = 'Fake' if votes['Fake'] > votes['Real'] else 'Real'\n",
    "                ensemble_confidence = float(max(votes.values()) / len(predictions))\n",
    "                \n",
    "                predictions['Ensemble'] = {\n",
    "                    'prediction': ensemble_prediction,\n",
    "                    'confidence': ensemble_confidence\n",
    "                }\n",
    "            \n",
    "            return predictions\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Prediction failed: {str(e)}\"}\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"Save trained models and preprocessors\"\"\"\n",
    "        model_data = {\n",
    "            'models': self.models,\n",
    "            'tfidf_vectorizer': self.tfidf_vectorizer,\n",
    "            'scaler': self.scaler,\n",
    "            'feature_names': self.feature_names\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with open(filepath, 'wb') as f:\n",
    "                pickle.dump(model_data, f)\n",
    "            print(f\"Models saved to {filepath}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving models: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"Load pre-trained models\"\"\"\n",
    "        try:\n",
    "            with open(filepath, 'rb') as f:\n",
    "                model_data = pickle.load(f)\n",
    "            \n",
    "            self.models = model_data['models']\n",
    "            self.tfidf_vectorizer = model_data['tfidf_vectorizer']\n",
    "            self.scaler = model_data['scaler']\n",
    "            self.feature_names = model_data.get('feature_names', [])\n",
    "            \n",
    "            print(f\"Models loaded from {filepath}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading models: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution with comprehensive error handling\"\"\"\n",
    "    print(\"=== FINAL FIXED FAKE NEWS DETECTION SYSTEM ===\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize detector\n",
    "        detector = FinalFakeNewsDetector()\n",
    "        \n",
    "        # Create expanded dataset\n",
    "        print(\"Creating expanded dataset...\")\n",
    "        df = detector.create_expanded_dataset()\n",
    "        print(f\"Dataset created: {len(df)} articles\")\n",
    "        print(f\"Real news: {sum(df['label'])}, Fake news: {len(df) - sum(df['label'])}\")\n",
    "        \n",
    "        # Prepare features\n",
    "        X, y = detector.prepare_features(df)\n",
    "        \n",
    "        # Split data with stratification\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, \n",
    "            test_size=0.3, \n",
    "            random_state=42, \n",
    "            stratify=y\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nData split completed:\")\n",
    "        print(f\"Training set: {len(X_train)} samples\")\n",
    "        print(f\"Test set: {len(X_test)} samples\")\n",
    "        print(f\"Training labels: Fake={list(y_train).count(0)}, Real={list(y_train).count(1)}\")\n",
    "        print(f\"Test labels: Fake={list(y_test).count(0)}, Real={list(y_test).count(1)}\")\n",
    "        \n",
    "        # Train models\n",
    "        success = detector.train_models(X_train, y_train)\n",
    "        \n",
    "        if not success:\n",
    "            print(\"Training failed. Exiting...\")\n",
    "            return None\n",
    "        \n",
    "        # Evaluate models\n",
    "        results = detector.evaluate_models(X_test, y_test)\n",
    "        \n",
    "        # Test with sample articles\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TESTING WITH SAMPLE ARTICLES\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        test_articles = [\n",
    "            \"Scientists at MIT have developed a revolutionary new battery technology according to peer-reviewed research published in Nature journal.\",\n",
    "            \"BREAKING: Government officials confirm alien contact, world leaders to make SHOCKING announcement tomorrow!!!\",\n",
    "            \"Local hospital reports successful treatment outcomes with new COVID-19 therapy in controlled clinical trial.\"\n",
    "        ]\n",
    "        \n",
    "        for i, article in enumerate(test_articles, 1):\n",
    "            print(f\"\\nTest Article {i}: {article[:80]}...\")\n",
    "            predictions = detector.predict_single_article(article)\n",
    "            \n",
    "            if \"error\" in predictions:\n",
    "                print(f\"Error: {predictions['error']}\")\n",
    "            else:\n",
    "                for model_name, result in predictions.items():\n",
    "                    print(f\"{model_name}: {result['prediction']} (Confidence: {result['confidence']:.3f})\")\n",
    "        \n",
    "        # Save models\n",
    "        detector.save_model('final_fake_news_detector.pkl')\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"SYSTEM TRAINING AND TESTING COMPLETE!\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        return detector\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"System error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    detector = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d66bf96-acf7-43cf-aa9c-677ed5d09484",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 23:12:36,601 - INFO - Creating comprehensive dataset...\n",
      "2025-09-06 23:12:36,606 - INFO - Dataset created: 50 articles\n",
      "2025-09-06 23:12:36,607 - INFO - Real news: 25, Fake news: 25\n",
      "2025-09-06 23:12:36,609 - INFO - Preprocessing and extracting enhanced features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ENHANCED FAKE NEWS DETECTION SYSTEM V2.0\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 23:12:38,999 - INFO - PCA applied: 30 components retained\n",
      "2025-09-06 23:12:39,000 - INFO - Final feature shape: (50, 30)\n",
      "2025-09-06 23:12:39,002 - INFO - Training set: 40 samples\n",
      "2025-09-06 23:12:39,003 - INFO - Test set: 10 samples\n",
      "2025-09-06 23:12:39,004 - INFO - Training models with cross-validation...\n",
      "2025-09-06 23:12:39,005 - INFO - Training Multinomial_NB...\n",
      "2025-09-06 23:12:44,255 - ERROR - Error training Multinomial_NB: \n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\envs\\Gen_AI\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\envs\\Gen_AI\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\admin\\anaconda3\\envs\\Gen_AI\\Lib\\site-packages\\sklearn\\naive_bayes.py\", line 762, in fit\n",
      "    self._count(X, Y)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\envs\\Gen_AI\\Lib\\site-packages\\sklearn\\naive_bayes.py\", line 889, in _count\n",
      "    check_non_negative(X, \"MultinomialNB (input X)\")\n",
      "  File \"C:\\Users\\admin\\anaconda3\\envs\\Gen_AI\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1824, in check_non_negative\n",
      "    raise ValueError(f\"Negative values in data passed to {whom}.\")\n",
      "ValueError: Negative values in data passed to MultinomialNB (input X).\n",
      "\n",
      "2025-09-06 23:12:44,256 - INFO - Training Logistic_Regression...\n",
      "2025-09-06 23:12:48,488 - INFO - Logistic_Regression - CV: 1.0000 (±0.0000)\n",
      "2025-09-06 23:12:48,490 - INFO - Training Random_Forest...\n",
      "2025-09-06 23:12:52,075 - INFO - Random_Forest - CV: 1.0000 (±0.0000)\n",
      "2025-09-06 23:12:52,076 - INFO - Training SVM...\n",
      "2025-09-06 23:12:52,157 - INFO - SVM - CV: 1.0000 (±0.0000)\n",
      "2025-09-06 23:12:52,158 - INFO - Training Gradient_Boosting...\n",
      "2025-09-06 23:12:52,452 - INFO - Gradient_Boosting - CV: 1.0000 (±0.0000)\n",
      "2025-09-06 23:12:52,453 - INFO - Ensemble model created successfully\n",
      "2025-09-06 23:12:52,453 - INFO - Comprehensive model evaluation...\n",
      "2025-09-06 23:12:52,476 - INFO - Logistic_Regression - Accuracy: 1.0000, F1: 1.0000, AUC: 1.0000\n",
      "2025-09-06 23:12:52,518 - INFO - Random_Forest - Accuracy: 1.0000, F1: 1.0000, AUC: 1.0000\n",
      "2025-09-06 23:12:52,532 - INFO - SVM - Accuracy: 1.0000, F1: 1.0000, AUC: 1.0000\n",
      "2025-09-06 23:12:52,545 - INFO - Gradient_Boosting - Accuracy: 1.0000, F1: 1.0000, AUC: 1.0000\n",
      "2025-09-06 23:12:52,546 - ERROR - Error evaluating ensemble: This VotingClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "2025-09-06 23:12:53,436 - INFO - Visualization report saved to reports/\n",
      "2025-09-06 23:12:53,437 - INFO - Preprocessing and extracting enhanced features...\n",
      "2025-09-06 23:12:53,442 - INFO - PCA applied: 30 components retained\n",
      "2025-09-06 23:12:53,443 - INFO - Final feature shape: (1, 30)\n",
      "2025-09-06 23:12:53,455 - ERROR - Error in ensemble prediction: This VotingClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "2025-09-06 23:12:53,458 - INFO - Preprocessing and extracting enhanced features...\n",
      "2025-09-06 23:12:53,463 - INFO - PCA applied: 30 components retained\n",
      "2025-09-06 23:12:53,464 - INFO - Final feature shape: (1, 30)\n",
      "2025-09-06 23:12:53,476 - ERROR - Error in ensemble prediction: This VotingClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "2025-09-06 23:12:53,478 - INFO - Preprocessing and extracting enhanced features...\n",
      "2025-09-06 23:12:53,483 - INFO - PCA applied: 30 components retained\n",
      "2025-09-06 23:12:53,484 - INFO - Final feature shape: (1, 30)\n",
      "2025-09-06 23:12:53,496 - ERROR - Error in ensemble prediction: This VotingClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "2025-09-06 23:12:53,499 - INFO - Preprocessing and extracting enhanced features...\n",
      "2025-09-06 23:12:53,504 - INFO - PCA applied: 30 components retained\n",
      "2025-09-06 23:12:53,505 - INFO - Final feature shape: (1, 30)\n",
      "2025-09-06 23:12:53,519 - ERROR - Error in ensemble prediction: This VotingClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "2025-09-06 23:12:53,522 - INFO - Preprocessing and extracting enhanced features...\n",
      "2025-09-06 23:12:53,529 - INFO - PCA applied: 30 components retained\n",
      "2025-09-06 23:12:53,530 - INFO - Final feature shape: (1, 30)\n",
      "2025-09-06 23:12:53,544 - ERROR - Error in ensemble prediction: This VotingClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "2025-09-06 23:12:53,562 - INFO - Enhanced model saved to enhanced_fake_news_detector_v2.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TESTING WITH ENHANCED PREDICTIONS\n",
      "============================================================\n",
      "\n",
      "Test Article 1: Scientists at MIT have developed a revolutionary new battery technology accordin...\n",
      "\n",
      "Predictions:\n",
      "  Logistic_Regression: Real (Confidence: 0.992)\n",
      "  Random_Forest: Real (Confidence: 0.710)\n",
      "  SVM: Real (Confidence: 0.984)\n",
      "  Gradient_Boosting: Real (Confidence: 1.000)\n",
      "  Consensus: Real (Confidence: 0.921)\n",
      "\n",
      "Explanation: This article is classified as REAL news with 92.1% confidence.\n",
      "Confidence Level: Very High\n",
      "Recommendation: Appears to be legitimate news, but still verify if sharing important information.\n",
      "\n",
      "Test Article 2: BREAKING: Government officials confirm alien contact, world leaders to make SHOC...\n",
      "\n",
      "Predictions:\n",
      "  Logistic_Regression: Fake (Confidence: 0.990)\n",
      "  Random_Forest: Fake (Confidence: 0.760)\n",
      "  SVM: Fake (Confidence: 0.980)\n",
      "  Gradient_Boosting: Fake (Confidence: 1.000)\n",
      "  Consensus: Fake (Confidence: 0.933)\n",
      "\n",
      "Explanation: This article is classified as FAKE news with 93.3% confidence.\n",
      "Confidence Level: Very High\n",
      "Key Indicators:\n",
      "  - Contains high emotional/sensational language\n",
      "  - Contains excessive exclamation marks\n",
      "Recommendation: Exercise extreme caution. Verify through multiple reliable sources before sharing.\n",
      "\n",
      "Test Article 3: Local hospital reports successful treatment outcomes with new COVID-19 therapy i...\n",
      "\n",
      "Predictions:\n",
      "  Logistic_Regression: Real (Confidence: 0.960)\n",
      "  Random_Forest: Real (Confidence: 0.880)\n",
      "  SVM: Real (Confidence: 0.990)\n",
      "  Gradient_Boosting: Real (Confidence: 1.000)\n",
      "  Consensus: Real (Confidence: 0.958)\n",
      "\n",
      "Explanation: This article is classified as REAL news with 95.8% confidence.\n",
      "Confidence Level: Very High\n",
      "Recommendation: Appears to be legitimate news, but still verify if sharing important information.\n",
      "\n",
      "Test Article 4: DOCTORS DON'T WANT YOU TO KNOW this ancient herbal remedy cures everything insta...\n",
      "\n",
      "Predictions:\n",
      "  Logistic_Regression: Fake (Confidence: 0.936)\n",
      "  Random_Forest: Fake (Confidence: 0.820)\n",
      "  SVM: Fake (Confidence: 0.980)\n",
      "  Gradient_Boosting: Fake (Confidence: 1.000)\n",
      "  Consensus: Fake (Confidence: 0.934)\n",
      "\n",
      "Explanation: This article is classified as FAKE news with 93.4% confidence.\n",
      "Confidence Level: Very High\n",
      "Recommendation: Exercise extreme caution. Verify through multiple reliable sources before sharing.\n",
      "\n",
      "Test Article 5: Federal Reserve announces interest rate changes following comprehensive economic...\n",
      "\n",
      "Predictions:\n",
      "  Logistic_Regression: Real (Confidence: 0.976)\n",
      "  Random_Forest: Real (Confidence: 0.820)\n",
      "  SVM: Real (Confidence: 0.992)\n",
      "  Gradient_Boosting: Real (Confidence: 1.000)\n",
      "  Consensus: Real (Confidence: 0.947)\n",
      "\n",
      "Explanation: This article is classified as REAL news with 94.7% confidence.\n",
      "Confidence Level: Very High\n",
      "Recommendation: Appears to be legitimate news, but still verify if sharing important information.\n",
      "\n",
      "============================================================\n",
      "ENHANCED SYSTEM TRAINING AND TESTING COMPLETE!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Fake News Detection System v2.0\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import json\n",
    "from textstat import flesch_reading_ease, flesch_kincaid_grade\n",
    "from collections import Counter\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('vader_lexicon', quiet=True)\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "class EnhancedFakeNewsDetector:\n",
    "    def __init__(self, config_file=None):\n",
    "        \"\"\"Initialize the Enhanced Fake News Detection System\"\"\"\n",
    "        self.config = self._load_config(config_file)\n",
    "        self.tfidf_vectorizer = None\n",
    "        self.count_vectorizer = None\n",
    "        self.scaler = None\n",
    "        self.pca = None\n",
    "        self.models = {}\n",
    "        self.ensemble_model = None\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.feature_names = []\n",
    "        self.feature_importance = {}\n",
    "        self.training_history = []\n",
    "        \n",
    "        # Advanced feature extractors\n",
    "        self.sentiment_analyzer = None\n",
    "        self._init_sentiment_analyzer()\n",
    "    \n",
    "    def _load_config(self, config_file):\n",
    "        \"\"\"Load configuration settings\"\"\"\n",
    "        default_config = {\n",
    "            'tfidf_max_features': 1000,\n",
    "            'tfidf_ngram_range': (1, 3),\n",
    "            'use_pca': True,\n",
    "            'pca_components': 0.95,\n",
    "            'cross_validation_folds': 5,\n",
    "            'random_state': 42,\n",
    "            'test_size': 0.2,\n",
    "            'min_word_length': 2,\n",
    "            'max_word_length': 20\n",
    "        }\n",
    "        \n",
    "        if config_file and os.path.exists(config_file):\n",
    "            with open(config_file, 'r') as f:\n",
    "                user_config = json.load(f)\n",
    "            default_config.update(user_config)\n",
    "        \n",
    "        return default_config\n",
    "    \n",
    "    def _init_sentiment_analyzer(self):\n",
    "        \"\"\"Initialize sentiment analysis tools\"\"\"\n",
    "        try:\n",
    "            from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "            self.sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "        except:\n",
    "            logger.warning(\"VADER sentiment analyzer not available\")\n",
    "    \n",
    "    def create_comprehensive_dataset(self):\n",
    "        \"\"\"Create a more comprehensive and diverse dataset\"\"\"\n",
    "        \n",
    "        # Expanded fake news examples with various patterns\n",
    "        fake_news = [\n",
    "            \"BREAKING: Scientists HATE this one weird trick that cures cancer instantly!\",\n",
    "            \"SHOCKING revelation: Government hiding alien technology for decades!\",\n",
    "            \"You won't BELIEVE what doctors found in this man's stomach!\",\n",
    "            \"URGENT: New study proves vaccines contain mind control chips!\",\n",
    "            \"INCREDIBLE: Local mom discovers miracle weight loss secret!\",\n",
    "            \"AMAZING: This simple trick will make you rich overnight!\",\n",
    "            \"UNBELIEVABLE: Child predicts future with 100% accuracy!\",\n",
    "            \"SECRET government plan to control weather revealed by whistleblower!\",\n",
    "            \"MIRACLE cure found in grandmother's attic eliminates all diseases!\",\n",
    "            \"TERRIFYING: 5G towers causing mass bird deaths worldwide!\",\n",
    "            \"EXCLUSIVE: Celebrity admits to being controlled by illuminati!\",\n",
    "            \"EXPOSED: Hospitals hiding cure for diabetes to make profits!\",\n",
    "            \"STUNNING: Man lives 200 years using this one simple method!\",\n",
    "            \"OUTRAGEOUS: Schools teaching children to worship Satan!\",\n",
    "            \"BOMBSHELL: Water fluoridation linked to zombie outbreak!\",\n",
    "            \"DOCTORS DON'T WANT YOU TO KNOW this ancient herbal remedy!\",\n",
    "            \"VIRAL: Woman cures arthritis with kitchen spice, pharma companies furious!\",\n",
    "            \"LEAKED: Government documents prove chemtrails are mind control experiments!\",\n",
    "            \"FORBIDDEN knowledge: How to unlock your psychic powers in 3 days!\",\n",
    "            \"CONSPIRACY: Major news outlets controlled by reptilian overlords!\",\n",
    "            \"ULTIMATE secret to eternal youth discovered by 90-year-old farmer!\",\n",
    "            \"MYSTERIOUS object found in Antarctica changes everything we know!\",\n",
    "            \"APOCALYPSE warning: Planet X approaching Earth, NASA covers up!\",\n",
    "            \"REVOLUTIONARY: Tap water turns toxic, only this filter can save you!\",\n",
    "            \"INCREDIBLE discovery: Lost city of Atlantis found using Google Maps!\"\n",
    "        ]\n",
    "        \n",
    "        # Expanded real news examples with credible patterns\n",
    "        real_news = [\n",
    "            \"Researchers at Stanford University published findings on renewable energy efficiency in peer-reviewed journal Nature.\",\n",
    "            \"The Federal Reserve announced a 0.25% interest rate increase following comprehensive economic data analysis.\",\n",
    "            \"Local hospital reports 15% decrease in COVID-19 cases over two-week period according to health officials.\",\n",
    "            \"NASA's James Webb telescope captures detailed images of distant galaxy formation in unprecedented detail.\",\n",
    "            \"City council approves $2.3 million budget for infrastructure improvements and public services enhancement.\",\n",
    "            \"University study shows meditation may reduce stress levels in clinical trial with 200 participants.\",\n",
    "            \"Environmental Protection Agency releases new guidelines for air quality monitoring in urban areas.\",\n",
    "            \"Medical researchers report promising results in early-stage cancer treatment trials at Johns Hopkins.\",\n",
    "            \"Economic indicators suggest moderate growth in manufacturing sector this quarter, analysts report.\",\n",
    "            \"Scientists at MIT develop new battery technology for electric vehicle applications in laboratory setting.\",\n",
    "            \"Health officials recommend updated vaccination schedules based on current epidemiological data.\",\n",
    "            \"Archaeological team discovers 3,000-year-old artifacts in systematic excavation near ancient city.\",\n",
    "            \"Climate research team publishes study on Arctic ice patterns in Journal of Climate Science.\",\n",
    "            \"University hospital announces successful organ transplant program expansion with improved outcomes.\",\n",
    "            \"Technology conference showcases advances in artificial intelligence applications for medical diagnosis.\",\n",
    "            \"Federal Trade Commission investigates merger between two major telecommunications companies for antitrust concerns.\",\n",
    "            \"International study involving 15 countries examines effectiveness of renewable energy policies over decade-long period.\",\n",
    "            \"Department of Education releases standardized test scores showing mixed results across different demographic groups.\",\n",
    "            \"Pharmaceutical company announces Phase III clinical trial results for new Alzheimer's treatment in medical journal.\",\n",
    "            \"Metropolitan transportation authority approves funding for electric bus fleet expansion in urban areas.\",\n",
    "            \"Agricultural researchers develop drought-resistant wheat varieties through selective breeding techniques at state university.\",\n",
    "            \"Housing market analysis shows regional variations in home prices according to National Association of Realtors.\",\n",
    "            \"Public health officials track seasonal flu patterns using data from hospital networks across multiple states.\",\n",
    "            \"Engineering team at technical institute creates more efficient solar panel design through materials research.\",\n",
    "            \"Central bank releases quarterly economic outlook based on employment statistics and inflation measurements.\"\n",
    "        ]\n",
    "        \n",
    "        # Create balanced dataset\n",
    "        texts = fake_news + real_news\n",
    "        labels = [0] * len(fake_news) + [1] * len(real_news)  # 0 = Fake, 1 = Real\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'text': texts,\n",
    "            'label': labels\n",
    "        })\n",
    "        \n",
    "        # Shuffle the dataset\n",
    "        df = df.sample(frac=1, random_state=self.config['random_state']).reset_index(drop=True)\n",
    "        \n",
    "        logger.info(f\"Dataset created: {len(df)} articles\")\n",
    "        logger.info(f\"Real news: {sum(df['label'])}, Fake news: {len(df) - sum(df['label'])}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def advanced_text_preprocessing(self, text):\n",
    "        \"\"\"Enhanced text preprocessing with multiple techniques\"\"\"\n",
    "        if pd.isna(text) or not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Store original for some features\n",
    "        original_text = text\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove URLs, emails, and social media handles\n",
    "        text = re.sub(r'http\\S+|www\\.\\S+|@\\w+|#\\w+', '', text)\n",
    "        \n",
    "        # Remove excessive punctuation but preserve some patterns\n",
    "        text = re.sub(r'[!]{2,}', '!!', text)  # Normalize multiple exclamations\n",
    "        text = re.sub(r'[?]{2,}', '??', text)  # Normalize multiple questions\n",
    "        text = re.sub(r'\\.{3,}', '...', text)  # Normalize ellipses\n",
    "        \n",
    "        # Remove special characters but keep important punctuation\n",
    "        text = re.sub(r'[^\\w\\s!?.,;:\\-\\'\"]', ' ', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Tokenize and filter\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Advanced filtering\n",
    "        processed_tokens = []\n",
    "        for token in tokens:\n",
    "            # Filter by length\n",
    "            if (len(token) >= self.config['min_word_length'] and \n",
    "                len(token) <= self.config['max_word_length']):\n",
    "                \n",
    "                if token.isalpha():\n",
    "                    # Only stem if not a potential indicator word\n",
    "                    if token not in ['breaking', 'urgent', 'shocking', 'amazing', 'incredible']:\n",
    "                        if token not in self.stop_words:\n",
    "                            processed_tokens.append(self.stemmer.stem(token))\n",
    "                        elif token in ['not', 'no', 'never']:  # Keep important negations\n",
    "                            processed_tokens.append(token)\n",
    "                    else:\n",
    "                        processed_tokens.append(token)\n",
    "                elif token in ['!', '?', '!!', '??']:\n",
    "                    processed_tokens.append(token)\n",
    "        \n",
    "        return ' '.join(processed_tokens)\n",
    "    \n",
    "    def extract_advanced_features(self, text, original_text=None):\n",
    "        \"\"\"Extract comprehensive linguistic and stylistic features\"\"\"\n",
    "        if original_text is None:\n",
    "            original_text = text\n",
    "            \n",
    "        features = {}\n",
    "        \n",
    "        if not text or len(text.strip()) == 0:\n",
    "            # Return zero features for empty text\n",
    "            feature_names = [\n",
    "                'char_count', 'word_count', 'sentence_count', 'avg_word_length',\n",
    "                'avg_sentence_length', 'exclamation_count', 'question_count',\n",
    "                'capital_ratio', 'exclamation_ratio', 'emotional_word_count',\n",
    "                'emotional_word_ratio', 'credibility_indicators', 'clickbait_count',\n",
    "                'caps_word_count', 'reading_ease', 'reading_grade', 'sentiment_pos',\n",
    "                'sentiment_neg', 'sentiment_neu', 'sentiment_compound', 'url_count',\n",
    "                'number_count', 'quoted_text_ratio', 'first_person_count',\n",
    "                'second_person_count', 'superlative_count', 'temporal_urgency',\n",
    "                'all_caps_ratio', 'punctuation_density'\n",
    "            ]\n",
    "            return {name: 0.0 for name in feature_names}\n",
    "        \n",
    "        words = text.split()\n",
    "        sentences = sent_tokenize(original_text)\n",
    "        \n",
    "        # Basic statistics\n",
    "        features['char_count'] = float(len(original_text))\n",
    "        features['word_count'] = float(len(words))\n",
    "        features['sentence_count'] = float(max(len(sentences), 1))\n",
    "        features['avg_word_length'] = float(np.mean([len(word) for word in words]) if words else 0)\n",
    "        features['avg_sentence_length'] = float(len(words) / max(len(sentences), 1))\n",
    "        \n",
    "        # Punctuation and style\n",
    "        features['exclamation_count'] = float(original_text.count('!'))\n",
    "        features['question_count'] = float(original_text.count('?'))\n",
    "        features['capital_ratio'] = float(sum(1 for c in original_text if c.isupper()) / max(len(original_text), 1))\n",
    "        features['exclamation_ratio'] = float(features['exclamation_count'] / max(len(sentences), 1))\n",
    "        \n",
    "        # Emotional and sensational language (expanded)\n",
    "        emotional_words = [\n",
    "            'amazing', 'shocking', 'unbelievable', 'incredible', 'outrageous', \n",
    "            'stunning', 'terrifying', 'miracle', 'secret', 'exposed', 'breaking',\n",
    "            'urgent', 'exclusive', 'revealed', 'discovered', 'bombshell', 'viral',\n",
    "            'forbidden', 'ultimate', 'revolutionary', 'mysterious', 'leaked',\n",
    "            'conspiracy', 'apocalypse', 'shocking', 'terrifying', 'incredible'\n",
    "        ]\n",
    "        emotional_count = sum(1 for word in emotional_words if word in text.lower())\n",
    "        features['emotional_word_count'] = float(emotional_count)\n",
    "        features['emotional_word_ratio'] = float(emotional_count / max(len(words), 1))\n",
    "        \n",
    "        # Credibility indicators (expanded)\n",
    "        credible_phrases = [\n",
    "            'according to', 'study shows', 'research indicates', 'scientists found',\n",
    "            'university', 'published', 'peer-reviewed', 'clinical trial', 'data suggests',\n",
    "            'researchers report', 'analysis reveals', 'evidence indicates', 'findings show',\n",
    "            'journal', 'institute', 'department', 'official', 'government', 'agency'\n",
    "        ]\n",
    "        features['credibility_indicators'] = float(sum(1 for phrase in credible_phrases if phrase in text.lower()))\n",
    "        \n",
    "        # Clickbait indicators (expanded)\n",
    "        clickbait_patterns = [\n",
    "            'you won\\'t believe', 'doctors hate', 'one weird trick', 'this will shock',\n",
    "            'what happened next', 'the results will surprise', 'number 7 will amaze',\n",
    "            'this simple trick', 'they don\\'t want you to know', 'will blow your mind'\n",
    "        ]\n",
    "        features['clickbait_count'] = float(sum(1 for phrase in clickbait_patterns if phrase in text.lower()))\n",
    "        \n",
    "        # All caps words\n",
    "        features['caps_word_count'] = float(sum(1 for word in words if word.isupper() and len(word) > 1))\n",
    "        features['all_caps_ratio'] = float(features['caps_word_count'] / max(len(words), 1))\n",
    "        \n",
    "        # Readability scores\n",
    "        try:\n",
    "            features['reading_ease'] = float(flesch_reading_ease(original_text))\n",
    "            features['reading_grade'] = float(flesch_kincaid_grade(original_text))\n",
    "        except:\n",
    "            features['reading_ease'] = 50.0  # Average readability\n",
    "            features['reading_grade'] = 8.0   # 8th grade level\n",
    "        \n",
    "        # Sentiment analysis\n",
    "        if self.sentiment_analyzer:\n",
    "            try:\n",
    "                scores = self.sentiment_analyzer.polarity_scores(original_text)\n",
    "                features['sentiment_pos'] = float(scores['pos'])\n",
    "                features['sentiment_neg'] = float(scores['neg'])\n",
    "                features['sentiment_neu'] = float(scores['neu'])\n",
    "                features['sentiment_compound'] = float(scores['compound'])\n",
    "            except:\n",
    "                features.update({\n",
    "                    'sentiment_pos': 0.0, 'sentiment_neg': 0.0,\n",
    "                    'sentiment_neu': 1.0, 'sentiment_compound': 0.0\n",
    "                })\n",
    "        else:\n",
    "            features.update({\n",
    "                'sentiment_pos': 0.0, 'sentiment_neg': 0.0,\n",
    "                'sentiment_neu': 1.0, 'sentiment_compound': 0.0\n",
    "            })\n",
    "        \n",
    "        # Additional features\n",
    "        features['url_count'] = float(len(re.findall(r'http\\S+|www\\.\\S+', original_text)))\n",
    "        features['number_count'] = float(len(re.findall(r'\\d+', original_text)))\n",
    "        features['quoted_text_ratio'] = float(original_text.count('\"') / max(len(original_text), 1))\n",
    "        \n",
    "        # Personal pronouns\n",
    "        first_person = ['i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours']\n",
    "        second_person = ['you', 'your', 'yours']\n",
    "        features['first_person_count'] = float(sum(1 for word in words if word.lower() in first_person))\n",
    "        features['second_person_count'] = float(sum(1 for word in words if word.lower() in second_person))\n",
    "        \n",
    "        # Superlatives\n",
    "        superlatives = ['best', 'worst', 'most', 'least', 'greatest', 'biggest', 'smallest']\n",
    "        features['superlative_count'] = float(sum(1 for word in words if word.lower() in superlatives))\n",
    "        \n",
    "        # Temporal urgency\n",
    "        urgent_temporal = ['now', 'today', 'immediately', 'urgent', 'breaking', 'just in']\n",
    "        features['temporal_urgency'] = float(sum(1 for phrase in urgent_temporal if phrase in text.lower()))\n",
    "        \n",
    "        # Punctuation density\n",
    "        punctuation_chars = '!?.,;:'\n",
    "        punctuation_count = sum(1 for char in original_text if char in punctuation_chars)\n",
    "        features['punctuation_density'] = float(punctuation_count / max(len(original_text), 1))\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def prepare_enhanced_features(self, df):\n",
    "        \"\"\"Prepare comprehensive feature set with advanced techniques\"\"\"\n",
    "        logger.info(\"Preprocessing and extracting enhanced features...\")\n",
    "        \n",
    "        # Advanced preprocessing\n",
    "        df['cleaned_text'] = df['text'].apply(self.advanced_text_preprocessing)\n",
    "        \n",
    "        # Extract advanced linguistic features\n",
    "        linguistic_features = []\n",
    "        for idx, row in df.iterrows():\n",
    "            features = self.extract_advanced_features(row['cleaned_text'], row['text'])\n",
    "            linguistic_features.append(features)\n",
    "        \n",
    "        linguistic_df = pd.DataFrame(linguistic_features)\n",
    "        self.feature_names = list(linguistic_df.columns)\n",
    "        \n",
    "        # Handle NaN values\n",
    "        linguistic_df = linguistic_df.fillna(0.0)\n",
    "        \n",
    "        # Robust scaling for linguistic features\n",
    "        if self.scaler is None:\n",
    "            self.scaler = RobustScaler()\n",
    "            linguistic_scaled = self.scaler.fit_transform(linguistic_df.values)\n",
    "        else:\n",
    "            linguistic_scaled = self.scaler.transform(linguistic_df.values)\n",
    "        \n",
    "        # TF-IDF vectorization\n",
    "        if self.tfidf_vectorizer is None:\n",
    "            self.tfidf_vectorizer = TfidfVectorizer(\n",
    "                max_features=self.config['tfidf_max_features'],\n",
    "                ngram_range=self.config['tfidf_ngram_range'],\n",
    "                min_df=2,\n",
    "                max_df=0.8,\n",
    "                stop_words='english',\n",
    "                sublinear_tf=True,\n",
    "                norm='l2'\n",
    "            )\n",
    "            tfidf_features = self.tfidf_vectorizer.fit_transform(df['cleaned_text'])\n",
    "        else:\n",
    "            tfidf_features = self.tfidf_vectorizer.transform(df['cleaned_text'])\n",
    "        \n",
    "        tfidf_dense = tfidf_features.toarray()\n",
    "        \n",
    "        # Count vectorization for additional features\n",
    "        if self.count_vectorizer is None:\n",
    "            self.count_vectorizer = CountVectorizer(\n",
    "                max_features=500,\n",
    "                ngram_range=(1, 2),\n",
    "                min_df=2,\n",
    "                max_df=0.8,\n",
    "                stop_words='english'\n",
    "            )\n",
    "            count_features = self.count_vectorizer.fit_transform(df['cleaned_text'])\n",
    "        else:\n",
    "            count_features = self.count_vectorizer.transform(df['cleaned_text'])\n",
    "        \n",
    "        count_dense = count_features.toarray()\n",
    "        \n",
    "        # Combine all features\n",
    "        combined_features = np.hstack([tfidf_dense, count_dense, linguistic_scaled])\n",
    "        \n",
    "        # Apply PCA for dimensionality reduction if configured\n",
    "        if self.config['use_pca']:\n",
    "            if self.pca is None:\n",
    "                self.pca = PCA(\n",
    "                    n_components=self.config['pca_components'],\n",
    "                    random_state=self.config['random_state']\n",
    "                )\n",
    "                combined_features = self.pca.fit_transform(combined_features)\n",
    "            else:\n",
    "                combined_features = self.pca.transform(combined_features)\n",
    "            \n",
    "            logger.info(f\"PCA applied: {combined_features.shape[1]} components retained\")\n",
    "        \n",
    "        logger.info(f\"Final feature shape: {combined_features.shape}\")\n",
    "        \n",
    "        return combined_features, df['label'].values\n",
    "    \n",
    "    def create_advanced_models(self):\n",
    "        \"\"\"Create advanced model ensemble with hyperparameter tuning\"\"\"\n",
    "        models = {\n",
    "            'Multinomial_NB': MultinomialNB(),\n",
    "            'Logistic_Regression': LogisticRegression(\n",
    "                random_state=self.config['random_state'],\n",
    "                max_iter=2000,\n",
    "                class_weight='balanced'\n",
    "            ),\n",
    "            'Random_Forest': RandomForestClassifier(\n",
    "                n_estimators=100,\n",
    "                random_state=self.config['random_state'],\n",
    "                class_weight='balanced',\n",
    "                max_depth=15\n",
    "            ),\n",
    "            'SVM': SVC(\n",
    "                random_state=self.config['random_state'],\n",
    "                class_weight='balanced',\n",
    "                probability=True,\n",
    "                kernel='rbf'\n",
    "            ),\n",
    "            'Gradient_Boosting': GradientBoostingClassifier(\n",
    "                n_estimators=100,\n",
    "                random_state=self.config['random_state'],\n",
    "                learning_rate=0.1\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        return models\n",
    "    \n",
    "    def train_with_cross_validation(self, X_train, y_train):\n",
    "        \"\"\"Train models with cross-validation and hyperparameter tuning\"\"\"\n",
    "        logger.info(\"Training models with cross-validation...\")\n",
    "        \n",
    "        models = self.create_advanced_models()\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            logger.info(f\"Training {name}...\")\n",
    "            \n",
    "            try:\n",
    "                # Cross-validation\n",
    "                cv_scores = cross_val_score(\n",
    "                    model, X_train, y_train,\n",
    "                    cv=self.config['cross_validation_folds'],\n",
    "                    scoring='accuracy',\n",
    "                    n_jobs=-1\n",
    "                )\n",
    "                \n",
    "                # Train final model\n",
    "                model.fit(X_train, y_train)\n",
    "                self.models[name] = model\n",
    "                \n",
    "                # Store training metrics\n",
    "                train_accuracy = accuracy_score(y_train, model.predict(X_train))\n",
    "                \n",
    "                training_info = {\n",
    "                    'model': name,\n",
    "                    'cv_mean': cv_scores.mean(),\n",
    "                    'cv_std': cv_scores.std(),\n",
    "                    'train_accuracy': train_accuracy\n",
    "                }\n",
    "                self.training_history.append(training_info)\n",
    "                \n",
    "                logger.info(f\"{name} - CV: {cv_scores.mean():.4f} (±{cv_scores.std():.4f})\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error training {name}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Create ensemble model\n",
    "        if len(self.models) >= 3:\n",
    "            self._create_ensemble_model()\n",
    "        \n",
    "        return len(self.models) > 0\n",
    "    \n",
    "    def _create_ensemble_model(self):\n",
    "        \"\"\"Create voting ensemble from trained models\"\"\"\n",
    "        try:\n",
    "            estimators = [(name, model) for name, model in self.models.items()]\n",
    "            \n",
    "            self.ensemble_model = VotingClassifier(\n",
    "                estimators=estimators,\n",
    "                voting='soft'\n",
    "            )\n",
    "            \n",
    "            # Note: ensemble is already \"fitted\" since individual models are fitted\n",
    "            logger.info(\"Ensemble model created successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating ensemble: {str(e)}\")\n",
    "    \n",
    "    def evaluate_comprehensive(self, X_test, y_test):\n",
    "        \"\"\"Comprehensive model evaluation with multiple metrics\"\"\"\n",
    "        logger.info(\"Comprehensive model evaluation...\")\n",
    "        \n",
    "        if not self.models:\n",
    "            logger.error(\"No models available for evaluation!\")\n",
    "            return {}\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            try:\n",
    "                y_pred = model.predict(X_test)\n",
    "                y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "                \n",
    "                # Multiple metrics\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                f1 = f1_score(y_test, y_pred)\n",
    "                auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "                \n",
    "                results[name] = {\n",
    "                    'accuracy': accuracy,\n",
    "                    'f1_score': f1,\n",
    "                    'auc_score': auc_score,\n",
    "                    'predictions': y_pred,\n",
    "                    'probabilities': y_pred_proba,\n",
    "                    'classification_report': classification_report(y_test, y_pred, zero_division=0)\n",
    "                }\n",
    "                \n",
    "                logger.info(f\"{name} - Accuracy: {accuracy:.4f}, F1: {f1:.4f}, AUC: {auc_score:.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error evaluating {name}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Evaluate ensemble if available\n",
    "        if self.ensemble_model:\n",
    "            try:\n",
    "                y_pred = self.ensemble_model.predict(X_test)\n",
    "                y_pred_proba = self.ensemble_model.predict_proba(X_test)[:, 1]\n",
    "                \n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                f1 = f1_score(y_test, y_pred)\n",
    "                auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "                \n",
    "                results['Ensemble'] = {\n",
    "                    'accuracy': accuracy,\n",
    "                    'f1_score': f1,\n",
    "                    'auc_score': auc_score,\n",
    "                    'predictions': y_pred,\n",
    "                    'probabilities': y_pred_proba,\n",
    "                    'classification_report': classification_report(y_test, y_pred, zero_division=0)\n",
    "                }\n",
    "                \n",
    "                logger.info(f\"Ensemble - Accuracy: {accuracy:.4f}, F1: {f1:.4f}, AUC: {auc_score:.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error evaluating ensemble: {str(e)}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def predict_with_confidence(self, text):\n",
    "        \"\"\"Make predictions with confidence intervals and explanations\"\"\"\n",
    "        if not self.models:\n",
    "            return {\"error\": \"No models available for prediction\"}\n",
    "        \n",
    "        try:\n",
    "            # Prepare features\n",
    "            temp_df = pd.DataFrame({'text': [text], 'label': [0]})\n",
    "            X, _ = self.prepare_enhanced_features(temp_df)\n",
    "            \n",
    "            predictions = {}\n",
    "            all_probabilities = []\n",
    "            \n",
    "            # Individual model predictions\n",
    "            for name, model in self.models.items():\n",
    "                try:\n",
    "                    pred = model.predict(X)[0]\n",
    "                    proba = model.predict_proba(X)[0]\n",
    "                    \n",
    "                    prediction_label = 'Real' if pred == 1 else 'Fake'\n",
    "                    confidence = float(max(proba))\n",
    "                    fake_prob = float(proba[0])\n",
    "                    real_prob = float(proba[1])\n",
    "                    \n",
    "                    predictions[name] = {\n",
    "                        'prediction': prediction_label,\n",
    "                        'confidence': confidence,\n",
    "                        'fake_probability': fake_prob,\n",
    "                        'real_probability': real_prob\n",
    "                    }\n",
    "                    \n",
    "                    all_probabilities.append([fake_prob, real_prob])\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error in prediction with {name}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            # Ensemble prediction if available\n",
    "            if self.ensemble_model:\n",
    "                try:\n",
    "                    pred = self.ensemble_model.predict(X)[0]\n",
    "                    proba = self.ensemble_model.predict_proba(X)[0]\n",
    "                    \n",
    "                    prediction_label = 'Real' if pred == 1 else 'Fake'\n",
    "                    confidence = float(max(proba))\n",
    "                    fake_prob = float(proba[0])\n",
    "                    real_prob = float(proba[1])\n",
    "                    \n",
    "                    predictions['Ensemble'] = {\n",
    "                        'prediction': prediction_label,\n",
    "                        'confidence': confidence,\n",
    "                        'fake_probability': fake_prob,\n",
    "                        'real_probability': real_prob\n",
    "                    }\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error in ensemble prediction: {str(e)}\")\n",
    "            \n",
    "            # Calculate consensus\n",
    "            if all_probabilities:\n",
    "                avg_probs = np.mean(all_probabilities, axis=0)\n",
    "                consensus_pred = 'Real' if avg_probs[1] > avg_probs[0] else 'Fake'\n",
    "                consensus_confidence = float(max(avg_probs))\n",
    "                \n",
    "                predictions['Consensus'] = {\n",
    "                    'prediction': consensus_pred,\n",
    "                    'confidence': consensus_confidence,\n",
    "                    'fake_probability': float(avg_probs[0]),\n",
    "                    'real_probability': float(avg_probs[1])\n",
    "                }\n",
    "            \n",
    "            # Add feature analysis\n",
    "            try:\n",
    "                features = self.extract_advanced_features(\n",
    "                    self.advanced_text_preprocessing(text), text\n",
    "                )\n",
    "                predictions['feature_analysis'] = self._analyze_key_features(features)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            return predictions\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Prediction failed: {str(e)}\"}\n",
    "    \n",
    "    def _analyze_key_features(self, features):\n",
    "        \"\"\"Analyze key features that influence the prediction\"\"\"\n",
    "        analysis = {}\n",
    "        \n",
    "        # Analyze key indicators\n",
    "        if features['emotional_word_ratio'] > 0.1:\n",
    "            analysis['high_emotional_content'] = True\n",
    "        \n",
    "        if features['clickbait_count'] > 0:\n",
    "            analysis['clickbait_detected'] = True\n",
    "        \n",
    "        if features['credibility_indicators'] > 2:\n",
    "            analysis['high_credibility_indicators'] = True\n",
    "        \n",
    "        if features['caps_word_count'] > 3:\n",
    "            analysis['excessive_capitalization'] = True\n",
    "        \n",
    "        if features['exclamation_ratio'] > 1:\n",
    "            analysis['excessive_exclamations'] = True\n",
    "        \n",
    "        # Readability analysis\n",
    "        if features['reading_ease'] < 30:\n",
    "            analysis['very_difficult_to_read'] = True\n",
    "        elif features['reading_ease'] > 90:\n",
    "            analysis['very_easy_to_read'] = True\n",
    "        \n",
    "        # Sentiment analysis\n",
    "        if features['sentiment_compound'] > 0.5:\n",
    "            analysis['highly_positive_sentiment'] = True\n",
    "        elif features['sentiment_compound'] < -0.5:\n",
    "            analysis['highly_negative_sentiment'] = True\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def generate_explanation(self, text, predictions):\n",
    "        \"\"\"Generate human-readable explanation for the prediction\"\"\"\n",
    "        try:\n",
    "            explanation = {\n",
    "                'summary': '',\n",
    "                'key_indicators': [],\n",
    "                'confidence_level': '',\n",
    "                'recommendation': ''\n",
    "            }\n",
    "            \n",
    "            # Get consensus prediction\n",
    "            if 'Consensus' in predictions:\n",
    "                main_pred = predictions['Consensus']['prediction']\n",
    "                main_conf = predictions['Consensus']['confidence']\n",
    "            elif 'Ensemble' in predictions:\n",
    "                main_pred = predictions['Ensemble']['prediction']\n",
    "                main_conf = predictions['Ensemble']['confidence']\n",
    "            else:\n",
    "                # Use most common prediction\n",
    "                preds = [p['prediction'] for p in predictions.values() if 'prediction' in p]\n",
    "                main_pred = max(set(preds), key=preds.count) if preds else 'Unknown'\n",
    "                main_conf = 0.5\n",
    "            \n",
    "            # Summary\n",
    "            explanation['summary'] = f\"This article is classified as {main_pred.upper()} news with {main_conf:.1%} confidence.\"\n",
    "            \n",
    "            # Confidence level\n",
    "            if main_conf >= 0.9:\n",
    "                explanation['confidence_level'] = 'Very High'\n",
    "            elif main_conf >= 0.75:\n",
    "                explanation['confidence_level'] = 'High'\n",
    "            elif main_conf >= 0.6:\n",
    "                explanation['confidence_level'] = 'Moderate'\n",
    "            else:\n",
    "                explanation['confidence_level'] = 'Low'\n",
    "            \n",
    "            # Key indicators from feature analysis\n",
    "            if 'feature_analysis' in predictions:\n",
    "                analysis = predictions['feature_analysis']\n",
    "                \n",
    "                if analysis.get('high_emotional_content'):\n",
    "                    explanation['key_indicators'].append(\"Contains high emotional/sensational language\")\n",
    "                \n",
    "                if analysis.get('clickbait_detected'):\n",
    "                    explanation['key_indicators'].append(\"Shows clickbait patterns\")\n",
    "                \n",
    "                if analysis.get('high_credibility_indicators'):\n",
    "                    explanation['key_indicators'].append(\"Contains credible source references\")\n",
    "                \n",
    "                if analysis.get('excessive_capitalization'):\n",
    "                    explanation['key_indicators'].append(\"Uses excessive capitalization\")\n",
    "                \n",
    "                if analysis.get('excessive_exclamations'):\n",
    "                    explanation['key_indicators'].append(\"Contains excessive exclamation marks\")\n",
    "            \n",
    "            # Recommendation\n",
    "            if main_pred == 'Fake' and main_conf > 0.7:\n",
    "                explanation['recommendation'] = \"Exercise extreme caution. Verify through multiple reliable sources before sharing.\"\n",
    "            elif main_pred == 'Fake':\n",
    "                explanation['recommendation'] = \"Be skeptical. Cross-check with established news sources.\"\n",
    "            elif main_pred == 'Real' and main_conf > 0.8:\n",
    "                explanation['recommendation'] = \"Appears to be legitimate news, but still verify if sharing important information.\"\n",
    "            else:\n",
    "                explanation['recommendation'] = \"Uncertain classification. Verify through reliable sources.\"\n",
    "            \n",
    "            return explanation\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Could not generate explanation: {str(e)}\"}\n",
    "    \n",
    "    def save_enhanced_model(self, filepath):\n",
    "        \"\"\"Save all components of the enhanced model\"\"\"\n",
    "        model_data = {\n",
    "            'models': self.models,\n",
    "            'ensemble_model': self.ensemble_model,\n",
    "            'tfidf_vectorizer': self.tfidf_vectorizer,\n",
    "            'count_vectorizer': self.count_vectorizer,\n",
    "            'scaler': self.scaler,\n",
    "            'pca': self.pca,\n",
    "            'feature_names': self.feature_names,\n",
    "            'config': self.config,\n",
    "            'training_history': self.training_history,\n",
    "            'feature_importance': self.feature_importance\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Create directory if it doesn't exist\n",
    "            Path(filepath).parent.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            with open(filepath, 'wb') as f:\n",
    "                pickle.dump(model_data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "            logger.info(f\"Enhanced model saved to {filepath}\")\n",
    "            \n",
    "            # Save metadata separately\n",
    "            metadata_path = filepath.replace('.pkl', '_metadata.json')\n",
    "            metadata = {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'model_count': len(self.models),\n",
    "                'feature_count': len(self.feature_names),\n",
    "                'config': self.config,\n",
    "                'training_history': self.training_history\n",
    "            }\n",
    "            \n",
    "            with open(metadata_path, 'w') as f:\n",
    "                json.dump(metadata, f, indent=2, default=str)\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving enhanced model: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def load_enhanced_model(self, filepath):\n",
    "        \"\"\"Load the enhanced model with all components\"\"\"\n",
    "        try:\n",
    "            with open(filepath, 'rb') as f:\n",
    "                model_data = pickle.load(f)\n",
    "            \n",
    "            self.models = model_data.get('models', {})\n",
    "            self.ensemble_model = model_data.get('ensemble_model')\n",
    "            self.tfidf_vectorizer = model_data.get('tfidf_vectorizer')\n",
    "            self.count_vectorizer = model_data.get('count_vectorizer')\n",
    "            self.scaler = model_data.get('scaler')\n",
    "            self.pca = model_data.get('pca')\n",
    "            self.feature_names = model_data.get('feature_names', [])\n",
    "            self.config.update(model_data.get('config', {}))\n",
    "            self.training_history = model_data.get('training_history', [])\n",
    "            self.feature_importance = model_data.get('feature_importance', {})\n",
    "            \n",
    "            logger.info(f\"Enhanced model loaded from {filepath}\")\n",
    "            logger.info(f\"Loaded {len(self.models)} individual models\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading enhanced model: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def create_visualization_report(self, results, output_dir='reports'):\n",
    "        \"\"\"Create comprehensive visualization report\"\"\"\n",
    "        try:\n",
    "            Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # Model comparison plot\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "            \n",
    "            # Accuracy comparison\n",
    "            models = list(results.keys())\n",
    "            accuracies = [results[model]['accuracy'] for model in models]\n",
    "            \n",
    "            axes[0, 0].bar(models, accuracies, color='skyblue')\n",
    "            axes[0, 0].set_title('Model Accuracy Comparison')\n",
    "            axes[0, 0].set_ylabel('Accuracy')\n",
    "            axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # F1-Score comparison\n",
    "            f1_scores = [results[model]['f1_score'] for model in models]\n",
    "            axes[0, 1].bar(models, f1_scores, color='lightcoral')\n",
    "            axes[0, 1].set_title('Model F1-Score Comparison')\n",
    "            axes[0, 1].set_ylabel('F1-Score')\n",
    "            axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # AUC comparison\n",
    "            auc_scores = [results[model]['auc_score'] for model in models]\n",
    "            axes[1, 0].bar(models, auc_scores, color='lightgreen')\n",
    "            axes[1, 0].set_title('Model AUC Score Comparison')\n",
    "            axes[1, 0].set_ylabel('AUC Score')\n",
    "            axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # Training history (if available)\n",
    "            if self.training_history:\n",
    "                cv_means = [item['cv_mean'] for item in self.training_history]\n",
    "                cv_stds = [item['cv_std'] for item in self.training_history]\n",
    "                model_names = [item['model'] for item in self.training_history]\n",
    "                \n",
    "                axes[1, 1].errorbar(range(len(model_names)), cv_means, yerr=cv_stds, \n",
    "                                  marker='o', capsize=5, capthick=2)\n",
    "                axes[1, 1].set_title('Cross-Validation Scores')\n",
    "                axes[1, 1].set_ylabel('CV Score')\n",
    "                axes[1, 1].set_xticks(range(len(model_names)))\n",
    "                axes[1, 1].set_xticklabels(model_names, rotation=45)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{output_dir}/model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # Generate HTML report\n",
    "            self._generate_html_report(results, output_dir)\n",
    "            \n",
    "            logger.info(f\"Visualization report saved to {output_dir}/\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating visualization report: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def _generate_html_report(self, results, output_dir):\n",
    "        \"\"\"Generate HTML report with detailed analysis\"\"\"\n",
    "        html_content = f\"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html>\n",
    "        <head>\n",
    "            <title>Fake News Detection Report</title>\n",
    "            <style>\n",
    "                body {{ font-family: Arial, sans-serif; margin: 40px; }}\n",
    "                .header {{ background-color: #f0f8ff; padding: 20px; border-radius: 10px; }}\n",
    "                .model-results {{ margin: 20px 0; padding: 15px; border: 1px solid #ddd; border-radius: 5px; }}\n",
    "                .metric {{ display: inline-block; margin: 10px; padding: 10px; background-color: #f9f9f9; border-radius: 5px; }}\n",
    "                .timestamp {{ color: #666; font-style: italic; }}\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <div class=\"header\">\n",
    "                <h1>Enhanced Fake News Detection System Report</h1>\n",
    "                <p class=\"timestamp\">Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\n",
    "            </div>\n",
    "            \n",
    "            <h2>Model Performance Summary</h2>\n",
    "        \"\"\"\n",
    "        \n",
    "        for model_name, result in results.items():\n",
    "            html_content += f\"\"\"\n",
    "            <div class=\"model-results\">\n",
    "                <h3>{model_name}</h3>\n",
    "                <div class=\"metric\">\n",
    "                    <strong>Accuracy:</strong> {result['accuracy']:.4f}\n",
    "                </div>\n",
    "                <div class=\"metric\">\n",
    "                    <strong>F1-Score:</strong> {result['f1_score']:.4f}\n",
    "                </div>\n",
    "                <div class=\"metric\">\n",
    "                    <strong>AUC Score:</strong> {result['auc_score']:.4f}\n",
    "                </div>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        \n",
    "        html_content += \"\"\"\n",
    "            <h2>Training Configuration</h2>\n",
    "            <ul>\n",
    "        \"\"\"\n",
    "        \n",
    "        for key, value in self.config.items():\n",
    "            html_content += f\"<li><strong>{key}:</strong> {value}</li>\"\n",
    "        \n",
    "        html_content += \"\"\"\n",
    "            </ul>\n",
    "            </body>\n",
    "            </html>\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(f'{output_dir}/report.html', 'w') as f:\n",
    "            f.write(html_content)\n",
    "\n",
    "def main_enhanced():\n",
    "    \"\"\"Enhanced main execution with comprehensive features\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ENHANCED FAKE NEWS DETECTION SYSTEM V2.0\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Initialize enhanced detector\n",
    "        detector = EnhancedFakeNewsDetector()\n",
    "        \n",
    "        # Create comprehensive dataset\n",
    "        logger.info(\"Creating comprehensive dataset...\")\n",
    "        df = detector.create_comprehensive_dataset()\n",
    "        \n",
    "        # Prepare enhanced features\n",
    "        X, y = detector.prepare_enhanced_features(df)\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y,\n",
    "            test_size=detector.config['test_size'],\n",
    "            random_state=detector.config['random_state'],\n",
    "            stratify=y\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Training set: {len(X_train)} samples\")\n",
    "        logger.info(f\"Test set: {len(X_test)} samples\")\n",
    "        \n",
    "        # Train with cross-validation\n",
    "        success = detector.train_with_cross_validation(X_train, y_train)\n",
    "        \n",
    "        if not success:\n",
    "            logger.error(\"Training failed. Exiting...\")\n",
    "            return None\n",
    "        \n",
    "        # Comprehensive evaluation\n",
    "        results = detector.evaluate_comprehensive(X_test, y_test)\n",
    "        \n",
    "        # Create visualization report\n",
    "        detector.create_visualization_report(results)\n",
    "        \n",
    "        # Test with enhanced predictions\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TESTING WITH ENHANCED PREDICTIONS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        test_articles = [\n",
    "            \"Scientists at MIT have developed a revolutionary new battery technology according to peer-reviewed research published in Nature journal.\",\n",
    "            \"BREAKING: Government officials confirm alien contact, world leaders to make SHOCKING announcement tomorrow!!!\",\n",
    "            \"Local hospital reports successful treatment outcomes with new COVID-19 therapy in controlled clinical trial.\",\n",
    "            \"DOCTORS DON'T WANT YOU TO KNOW this ancient herbal remedy cures everything instantly!\",\n",
    "            \"Federal Reserve announces interest rate changes following comprehensive economic analysis by board members.\"\n",
    "        ]\n",
    "        \n",
    "        for i, article in enumerate(test_articles, 1):\n",
    "            print(f\"\\nTest Article {i}: {article[:80]}...\")\n",
    "            predictions = detector.predict_with_confidence(article)\n",
    "            explanation = detector.generate_explanation(article, predictions)\n",
    "            \n",
    "            if \"error\" in predictions:\n",
    "                print(f\"Error: {predictions['error']}\")\n",
    "            else:\n",
    "                print(\"\\nPredictions:\")\n",
    "                for model_name, result in predictions.items():\n",
    "                    if 'prediction' in result:\n",
    "                        print(f\"  {model_name}: {result['prediction']} \"\n",
    "                              f\"(Confidence: {result['confidence']:.3f})\")\n",
    "                \n",
    "                print(f\"\\nExplanation: {explanation.get('summary', 'N/A')}\")\n",
    "                print(f\"Confidence Level: {explanation.get('confidence_level', 'N/A')}\")\n",
    "                if explanation.get('key_indicators'):\n",
    "                    print(\"Key Indicators:\")\n",
    "                    for indicator in explanation['key_indicators']:\n",
    "                        print(f\"  - {indicator}\")\n",
    "                print(f\"Recommendation: {explanation.get('recommendation', 'N/A')}\")\n",
    "        \n",
    "        # Save enhanced model\n",
    "        detector.save_enhanced_model('enhanced_fake_news_detector_v2.pkl')\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ENHANCED SYSTEM TRAINING AND TESTING COMPLETE!\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        return detector\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"System error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    detector = main_enhanced()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9e5ce50-6540-4c80-8174-c7e740576413",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 23:53:33,350 - INFO - Creating comprehensive dataset...\n",
      "2025-09-06 23:53:33,353 - INFO - Dataset created: 50 articles\n",
      "2025-09-06 23:53:33,354 - INFO - Real news: 25, Fake news: 25\n",
      "2025-09-06 23:53:33,355 - INFO - Preprocessing and extracting enhanced features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Running Enhanced Fake News Detection System v2.0...\n",
      "============================================================\n",
      "ENHANCED FAKE NEWS DETECTION SYSTEM V2.0\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 23:53:34,314 - INFO - PCA applied: 30 components retained\n",
      "2025-09-06 23:53:34,315 - INFO - Final feature shape: (50, 30)\n",
      "2025-09-06 23:53:34,318 - INFO - Training set: 40 samples\n",
      "2025-09-06 23:53:34,319 - INFO - Test set: 10 samples\n",
      "2025-09-06 23:53:34,320 - INFO - Training models with cross-validation...\n",
      "2025-09-06 23:53:34,321 - INFO - Training Multinomial_NB...\n",
      "2025-09-06 23:53:41,131 - ERROR - Error training Multinomial_NB: \n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\Gen_AI\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\Gen_AI\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\Gen_AI\\Lib\\site-packages\\sklearn\\naive_bayes.py\", line 762, in fit\n",
      "    self._count(X, Y)\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\Gen_AI\\Lib\\site-packages\\sklearn\\naive_bayes.py\", line 889, in _count\n",
      "    check_non_negative(X, \"MultinomialNB (input X)\")\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\Gen_AI\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1824, in check_non_negative\n",
      "    raise ValueError(f\"Negative values in data passed to {whom}.\")\n",
      "ValueError: Negative values in data passed to MultinomialNB (input X).\n",
      "\n",
      "2025-09-06 23:53:41,134 - INFO - Training Logistic_Regression...\n",
      "2025-09-06 23:53:46,361 - INFO - Logistic_Regression - CV: 1.0000 (±0.0000)\n",
      "2025-09-06 23:53:46,362 - INFO - Training Random_Forest...\n",
      "2025-09-06 23:53:53,199 - INFO - Random_Forest - CV: 1.0000 (±0.0000)\n",
      "2025-09-06 23:53:53,200 - INFO - Training SVM...\n",
      "2025-09-06 23:53:53,271 - INFO - SVM - CV: 1.0000 (±0.0000)\n",
      "2025-09-06 23:53:53,271 - INFO - Training Gradient_Boosting...\n",
      "2025-09-06 23:53:53,601 - INFO - Gradient_Boosting - CV: 1.0000 (±0.0000)\n",
      "2025-09-06 23:53:53,602 - INFO - Ensemble model created successfully\n",
      "2025-09-06 23:53:53,603 - INFO - Comprehensive model evaluation...\n",
      "2025-09-06 23:53:53,614 - INFO - Logistic_Regression - Accuracy: 1.0000, F1: 1.0000, AUC: 1.0000\n",
      "2025-09-06 23:53:53,636 - INFO - Random_Forest - Accuracy: 1.0000, F1: 1.0000, AUC: 1.0000\n",
      "2025-09-06 23:53:53,651 - INFO - SVM - Accuracy: 1.0000, F1: 1.0000, AUC: 1.0000\n",
      "2025-09-06 23:53:53,663 - INFO - Gradient_Boosting - Accuracy: 1.0000, F1: 1.0000, AUC: 1.0000\n",
      "2025-09-06 23:53:53,664 - ERROR - Error evaluating ensemble: This VotingClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "2025-09-06 23:53:54,662 - INFO - Visualization report saved to reports/\n",
      "2025-09-06 23:53:54,664 - INFO - Preprocessing and extracting enhanced features...\n",
      "2025-09-06 23:53:54,669 - INFO - PCA applied: 30 components retained\n",
      "2025-09-06 23:53:54,671 - INFO - Final feature shape: (1, 30)\n",
      "2025-09-06 23:53:54,686 - ERROR - Error in ensemble prediction: This VotingClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "2025-09-06 23:53:54,689 - INFO - Preprocessing and extracting enhanced features...\n",
      "2025-09-06 23:53:54,695 - INFO - PCA applied: 30 components retained\n",
      "2025-09-06 23:53:54,696 - INFO - Final feature shape: (1, 30)\n",
      "2025-09-06 23:53:54,711 - ERROR - Error in ensemble prediction: This VotingClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "2025-09-06 23:53:54,713 - INFO - Preprocessing and extracting enhanced features...\n",
      "2025-09-06 23:53:54,718 - INFO - PCA applied: 30 components retained\n",
      "2025-09-06 23:53:54,719 - INFO - Final feature shape: (1, 30)\n",
      "2025-09-06 23:53:54,733 - ERROR - Error in ensemble prediction: This VotingClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "2025-09-06 23:53:54,737 - INFO - Preprocessing and extracting enhanced features...\n",
      "2025-09-06 23:53:54,743 - INFO - PCA applied: 30 components retained\n",
      "2025-09-06 23:53:54,743 - INFO - Final feature shape: (1, 30)\n",
      "2025-09-06 23:53:54,761 - ERROR - Error in ensemble prediction: This VotingClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "2025-09-06 23:53:54,764 - INFO - Preprocessing and extracting enhanced features...\n",
      "2025-09-06 23:53:54,768 - INFO - PCA applied: 30 components retained\n",
      "2025-09-06 23:53:54,770 - INFO - Final feature shape: (1, 30)\n",
      "2025-09-06 23:53:54,783 - ERROR - Error in ensemble prediction: This VotingClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "2025-09-06 23:53:54,798 - INFO - Enhanced model saved to enhanced_fake_news_detector_v2.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TESTING WITH ENHANCED PREDICTIONS\n",
      "============================================================\n",
      "\n",
      "Test Article 1: Scientists at MIT have developed a revolutionary new battery technology accordin...\n",
      "\n",
      "Predictions:\n",
      "  Logistic_Regression: Real (Confidence: 0.992)\n",
      "  Random_Forest: Real (Confidence: 0.710)\n",
      "  SVM: Real (Confidence: 0.984)\n",
      "  Gradient_Boosting: Real (Confidence: 1.000)\n",
      "  Consensus: Real (Confidence: 0.921)\n",
      "\n",
      "Explanation: This article is classified as REAL news with 92.1% confidence.\n",
      "Confidence Level: Very High\n",
      "Recommendation: Appears to be legitimate news, but still verify if sharing important information.\n",
      "\n",
      "Test Article 2: BREAKING: Government officials confirm alien contact, world leaders to make SHOC...\n",
      "\n",
      "Predictions:\n",
      "  Logistic_Regression: Fake (Confidence: 0.990)\n",
      "  Random_Forest: Fake (Confidence: 0.760)\n",
      "  SVM: Fake (Confidence: 0.980)\n",
      "  Gradient_Boosting: Fake (Confidence: 1.000)\n",
      "  Consensus: Fake (Confidence: 0.933)\n",
      "\n",
      "Explanation: This article is classified as FAKE news with 93.3% confidence.\n",
      "Confidence Level: Very High\n",
      "Key Indicators:\n",
      "  - Contains high emotional/sensational language\n",
      "  - Contains excessive exclamation marks\n",
      "Recommendation: Exercise extreme caution. Verify through multiple reliable sources before sharing.\n",
      "\n",
      "Test Article 3: Local hospital reports successful treatment outcomes with new COVID-19 therapy i...\n",
      "\n",
      "Predictions:\n",
      "  Logistic_Regression: Real (Confidence: 0.960)\n",
      "  Random_Forest: Real (Confidence: 0.880)\n",
      "  SVM: Real (Confidence: 0.990)\n",
      "  Gradient_Boosting: Real (Confidence: 1.000)\n",
      "  Consensus: Real (Confidence: 0.958)\n",
      "\n",
      "Explanation: This article is classified as REAL news with 95.8% confidence.\n",
      "Confidence Level: Very High\n",
      "Recommendation: Appears to be legitimate news, but still verify if sharing important information.\n",
      "\n",
      "Test Article 4: DOCTORS DON'T WANT YOU TO KNOW this ancient herbal remedy cures everything insta...\n",
      "\n",
      "Predictions:\n",
      "  Logistic_Regression: Fake (Confidence: 0.936)\n",
      "  Random_Forest: Fake (Confidence: 0.820)\n",
      "  SVM: Fake (Confidence: 0.980)\n",
      "  Gradient_Boosting: Fake (Confidence: 1.000)\n",
      "  Consensus: Fake (Confidence: 0.934)\n",
      "\n",
      "Explanation: This article is classified as FAKE news with 93.4% confidence.\n",
      "Confidence Level: Very High\n",
      "Recommendation: Exercise extreme caution. Verify through multiple reliable sources before sharing.\n",
      "\n",
      "Test Article 5: Federal Reserve announces interest rate changes following comprehensive economic...\n",
      "\n",
      "Predictions:\n",
      "  Logistic_Regression: Real (Confidence: 0.976)\n",
      "  Random_Forest: Real (Confidence: 0.820)\n",
      "  SVM: Real (Confidence: 0.992)\n",
      "  Gradient_Boosting: Real (Confidence: 1.000)\n",
      "  Consensus: Real (Confidence: 0.947)\n",
      "\n",
      "Explanation: This article is classified as REAL news with 94.7% confidence.\n",
      "Confidence Level: Very High\n",
      "Recommendation: Appears to be legitimate news, but still verify if sharing important information.\n",
      "\n",
      "============================================================\n",
      "ENHANCED SYSTEM TRAINING AND TESTING COMPLETE!\n",
      "============================================================\n",
      "\n",
      "✅ System initialized successfully!\n",
      "📊 Check the 'reports' folder for visualization reports\n",
      "💾 Model saved as 'enhanced_fake_news_detector_v2.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install required packages and run the enhanced system\n",
    "# Copy this code and run it in your Python environment\n",
    "\n",
    "# First, install additional required packages:\n",
    "\"\"\"\n",
    "pip install textstat\n",
    "pip install matplotlib\n",
    "pip install seaborn\n",
    "\"\"\"\n",
    "\n",
    "# Then run the enhanced system\n",
    "from enhanced_fake_news_detector_v2 import main_enhanced\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 Running Enhanced Fake News Detection System v2.0...\")\n",
    "    detector = main_enhanced()\n",
    "    \n",
    "    if detector:\n",
    "        print(\"\\n✅ System initialized successfully!\")\n",
    "        print(\"📊 Check the 'reports' folder for visualization reports\")\n",
    "        print(\"💾 Model saved as 'enhanced_fake_news_detector_v2.pkl'\")\n",
    "    else:\n",
    "        print(\"❌ System failed to initialize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f62d739-64e7-4331-8db8-99a62eea6f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 23:54:07,223 - INFO - Enhanced model loaded from enhanced_fake_news_detector_v2.pkl\n",
      "2025-09-06 23:54:07,225 - INFO - Loaded 4 individual models\n",
      "2025-09-06 23:54:07,227 - INFO - Preprocessing and extracting enhanced features...\n",
      "2025-09-06 23:54:07,237 - INFO - PCA applied: 30 components retained\n",
      "2025-09-06 23:54:07,238 - INFO - Final feature shape: (1, 30)\n",
      "2025-09-06 23:54:07,249 - ERROR - Error in ensemble prediction: This VotingClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "2025-09-06 23:54:07,252 - INFO - Preprocessing and extracting enhanced features...\n",
      "2025-09-06 23:54:07,259 - INFO - PCA applied: 30 components retained\n",
      "2025-09-06 23:54:07,260 - INFO - Final feature shape: (1, 30)\n",
      "2025-09-06 23:54:07,272 - ERROR - Error in ensemble prediction: This VotingClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "2025-09-06 23:54:07,275 - INFO - Preprocessing and extracting enhanced features...\n",
      "2025-09-06 23:54:07,279 - INFO - PCA applied: 30 components retained\n",
      "2025-09-06 23:54:07,280 - INFO - Final feature shape: (1, 30)\n",
      "2025-09-06 23:54:07,291 - ERROR - Error in ensemble prediction: This VotingClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "2025-09-06 23:54:07,294 - INFO - Preprocessing and extracting enhanced features...\n",
      "2025-09-06 23:54:07,298 - INFO - PCA applied: 30 components retained\n",
      "2025-09-06 23:54:07,300 - INFO - Final feature shape: (1, 30)\n",
      "2025-09-06 23:54:07,311 - ERROR - Error in ensemble prediction: This VotingClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "2025-09-06 23:54:07,314 - INFO - Preprocessing and extracting enhanced features...\n",
      "2025-09-06 23:54:07,318 - INFO - PCA applied: 30 components retained\n",
      "2025-09-06 23:54:07,320 - INFO - Final feature shape: (1, 30)\n",
      "2025-09-06 23:54:07,329 - ERROR - Error in ensemble prediction: This VotingClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Testing Enhanced Fake News Detection System\n",
      "============================================================\n",
      "\n",
      "📰 Test Article 1: Credible Scientific Article\n",
      "Text: Researchers at Harvard Medical School published a peer-reviewed study in the Journal of Medical Rese...\n",
      "\n",
      "🤖 AI Predictions:\n",
      "  ✅ Logistic_Regression: Real (100.0% confidence)\n",
      "  ✅ Random_Forest: Real (76.0% confidence)\n",
      "  ✅ SVM: Real (77.1% confidence)\n",
      "  ✅ Gradient_Boosting: Real (100.0% confidence)\n",
      "  ✅ Consensus: Real (88.3% confidence)\n",
      "\n",
      "💡 Analysis: This article is classified as REAL news with 88.3% confidence.\n",
      "🎯 Confidence Level: High\n",
      "💬 Recommendation: Appears to be legitimate news, but still verify if sharing important information.\n",
      "------------------------------------------------------------\n",
      "\n",
      "📰 Test Article 2: Obvious Fake News\n",
      "Text: BREAKING!!! Scientists SHOCKED by this ONE WEIRD TRICK that INSTANTLY cures diabetes! Doctors HATE t...\n",
      "\n",
      "🤖 AI Predictions:\n",
      "  ⚠️ Logistic_Regression: Fake (99.9% confidence)\n",
      "  ⚠️ Random_Forest: Fake (58.0% confidence)\n",
      "  ⚠️ SVM: Fake (78.3% confidence)\n",
      "  ⚠️ Gradient_Boosting: Fake (100.0% confidence)\n",
      "  ⚠️ Consensus: Fake (84.1% confidence)\n",
      "\n",
      "💡 Analysis: This article is classified as FAKE news with 84.1% confidence.\n",
      "🎯 Confidence Level: High\n",
      "🔍 Key Indicators:\n",
      "   • Shows clickbait patterns\n",
      "   • Contains excessive exclamation marks\n",
      "💬 Recommendation: Exercise extreme caution. Verify through multiple reliable sources before sharing.\n",
      "------------------------------------------------------------\n",
      "\n",
      "📰 Test Article 3: Neutral News Report\n",
      "Text: The Federal Reserve announced today a decision to maintain current interest rates at 5.25% following...\n",
      "\n",
      "🤖 AI Predictions:\n",
      "  ✅ Logistic_Regression: Real (100.0% confidence)\n",
      "  ✅ Random_Forest: Real (86.0% confidence)\n",
      "  ✅ SVM: Real (87.3% confidence)\n",
      "  ✅ Gradient_Boosting: Real (100.0% confidence)\n",
      "  ✅ Consensus: Real (93.3% confidence)\n",
      "\n",
      "💡 Analysis: This article is classified as REAL news with 93.3% confidence.\n",
      "🎯 Confidence Level: Very High\n",
      "💬 Recommendation: Appears to be legitimate news, but still verify if sharing important information.\n",
      "------------------------------------------------------------\n",
      "\n",
      "📰 Test Article 4: Suspicious Health Claim\n",
      "Text: INCREDIBLE discovery: This ancient herb from the Amazon rainforest can allegedly cure ANY disease wi...\n",
      "\n",
      "🤖 AI Predictions:\n",
      "  ✅ Logistic_Regression: Real (64.3% confidence)\n",
      "  ✅ Random_Forest: Real (78.0% confidence)\n",
      "  ⚠️ SVM: Fake (52.2% confidence)\n",
      "  ✅ Gradient_Boosting: Real (100.0% confidence)\n",
      "  ✅ Consensus: Real (73.6% confidence)\n",
      "\n",
      "💡 Analysis: This article is classified as REAL news with 73.6% confidence.\n",
      "🎯 Confidence Level: Moderate\n",
      "🔍 Key Indicators:\n",
      "   • Contains high emotional/sensational language\n",
      "💬 Recommendation: Uncertain classification. Verify through reliable sources.\n",
      "------------------------------------------------------------\n",
      "\n",
      "📰 Test Article 5: Legitimate Technology News\n",
      "Text: Apple Inc. reported quarterly earnings that exceeded analyst expectations, with iPhone sales contrib...\n",
      "\n",
      "🤖 AI Predictions:\n",
      "  ✅ Logistic_Regression: Real (100.0% confidence)\n",
      "  ✅ Random_Forest: Real (87.0% confidence)\n",
      "  ✅ SVM: Real (95.6% confidence)\n",
      "  ✅ Gradient_Boosting: Real (100.0% confidence)\n",
      "  ✅ Consensus: Real (95.6% confidence)\n",
      "\n",
      "💡 Analysis: This article is classified as REAL news with 95.6% confidence.\n",
      "🎯 Confidence Level: Very High\n",
      "💬 Recommendation: Appears to be legitimate news, but still verify if sharing important information.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Test the enhanced system with real articles\n",
    "from enhanced_fake_news_detector_v2 import EnhancedFakeNewsDetector\n",
    "import json\n",
    "\n",
    "def test_with_real_articles():\n",
    "    \"\"\"Test the system with various types of articles\"\"\"\n",
    "    \n",
    "    # Load the trained model\n",
    "    detector = EnhancedFakeNewsDetector()\n",
    "    success = detector.load_enhanced_model('enhanced_fake_news_detector_v2.pkl')\n",
    "    \n",
    "    if not success:\n",
    "        print(\"❌ Could not load the model. Please run the training first.\")\n",
    "        return\n",
    "    \n",
    "    # Test articles from different categories\n",
    "    test_articles = [\n",
    "        {\n",
    "            'title': 'Credible Scientific Article',\n",
    "            'text': 'Researchers at Harvard Medical School published a peer-reviewed study in the Journal of Medical Research showing that a Mediterranean diet may reduce cardiovascular disease risk by 30% in a randomized controlled trial with 7,500 participants over five years.'\n",
    "        },\n",
    "        {\n",
    "            'title': 'Obvious Fake News',\n",
    "            'text': 'BREAKING!!! Scientists SHOCKED by this ONE WEIRD TRICK that INSTANTLY cures diabetes! Doctors HATE this simple method discovered by a grandmother! Big Pharma is trying to HIDE this MIRACLE cure! Click NOW before it\\'s BANNED!!!'\n",
    "        },\n",
    "        {\n",
    "            'title': 'Neutral News Report',\n",
    "            'text': 'The Federal Reserve announced today a decision to maintain current interest rates at 5.25% following their monthly meeting. The decision was based on recent economic indicators including employment data and inflation measurements, according to Fed Chairman statements.'\n",
    "        },\n",
    "        {\n",
    "            'title': 'Suspicious Health Claim',\n",
    "            'text': 'INCREDIBLE discovery: This ancient herb from the Amazon rainforest can allegedly cure ANY disease within 24 hours! Thousands of people are claiming AMAZING results! The medical establishment doesn\\'t want you to know about this POWERFUL secret!'\n",
    "        },\n",
    "        {\n",
    "            'title': 'Legitimate Technology News',\n",
    "            'text': 'Apple Inc. reported quarterly earnings that exceeded analyst expectations, with iPhone sales contributing to a 12% revenue increase compared to the same period last year. The company also announced plans to expand manufacturing operations in Southeast Asia.'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"🔍 Testing Enhanced Fake News Detection System\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, article in enumerate(test_articles, 1):\n",
    "        print(f\"\\n📰 Test Article {i}: {article['title']}\")\n",
    "        print(f\"Text: {article['text'][:100]}...\")\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = detector.predict_with_confidence(article['text'])\n",
    "        explanation = detector.generate_explanation(article['text'], predictions)\n",
    "        \n",
    "        if \"error\" in predictions:\n",
    "            print(f\"❌ Error: {predictions['error']}\")\n",
    "            continue\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\n🤖 AI Predictions:\")\n",
    "        for model_name, result in predictions.items():\n",
    "            if 'prediction' in result:\n",
    "                emoji = \"✅\" if result['prediction'] == 'Real' else \"⚠️\"\n",
    "                print(f\"  {emoji} {model_name}: {result['prediction']} \"\n",
    "                      f\"({result['confidence']:.1%} confidence)\")\n",
    "        \n",
    "        # Display explanation\n",
    "        if explanation and not 'error' in explanation:\n",
    "            print(f\"\\n💡 Analysis: {explanation.get('summary', 'N/A')}\")\n",
    "            print(f\"🎯 Confidence Level: {explanation.get('confidence_level', 'N/A')}\")\n",
    "            \n",
    "            if explanation.get('key_indicators'):\n",
    "                print(\"🔍 Key Indicators:\")\n",
    "                for indicator in explanation['key_indicators']:\n",
    "                    print(f\"   • {indicator}\")\n",
    "            \n",
    "            print(f\"💬 Recommendation: {explanation.get('recommendation', 'N/A')}\")\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "\n",
    "def interactive_testing():\n",
    "    \"\"\"Interactive testing where user can input their own articles\"\"\"\n",
    "    \n",
    "    detector = EnhancedFakeNewsDetector()\n",
    "    success = detector.load_enhanced_model('enhanced_fake_news_detector_v2.pkl')\n",
    "    \n",
    "    if not success:\n",
    "        print(\"❌ Could not load the model. Please run the training first.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n🎮 Interactive Fake News Detection\")\n",
    "    print(\"Enter articles to analyze (type 'quit' to exit)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\n📝 Enter article text: \").strip()\n",
    "        \n",
    "        if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "            break\n",
    "        \n",
    "        if len(user_input) < 10:\n",
    "            print(\"⚠️ Please enter a longer article (at least 10 characters)\")\n",
    "            continue\n",
    "        \n",
    "        # Analyze the article\n",
    "        predictions = detector.predict_with_confidence(user_input)\n",
    "        explanation = detector.generate_explanation(user_input, predictions)\n",
    "        \n",
    "        if \"error\" in predictions:\n",
    "            print(f\"❌ Error: {predictions['error']}\")\n",
    "            continue\n",
    "        \n",
    "        # Show results\n",
    "        print(\"\\n🔍 Analysis Results:\")\n",
    "        \n",
    "        # Get consensus prediction\n",
    "        if 'Consensus' in predictions:\n",
    "            main_result = predictions['Consensus']\n",
    "            emoji = \"✅\" if main_result['prediction'] == 'Real' else \"⚠️\"\n",
    "            print(f\"{emoji} Overall Assessment: {main_result['prediction']} \"\n",
    "                  f\"({main_result['confidence']:.1%} confidence)\")\n",
    "        \n",
    "        # Show explanation\n",
    "        if explanation and 'error' not in explanation:\n",
    "            print(f\"💡 {explanation.get('summary', '')}\")\n",
    "            print(f\"💬 {explanation.get('recommendation', '')}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run automated tests\n",
    "    test_with_real_articles()\n",
    "    \n",
    "    # Uncomment the line below for interactive testing\n",
    "    # interactive_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebf12a9e-ef07-4272-9208-a3bac88a20d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 23:54:28,393 - INFO - Creating comprehensive dataset...\n",
      "2025-09-06 23:54:28,396 - INFO - Dataset created: 50 articles\n",
      "2025-09-06 23:54:28,397 - INFO - Real news: 25, Fake news: 25\n",
      "2025-09-06 23:54:28,398 - INFO - Preprocessing and extracting enhanced features...\n",
      "2025-09-06 23:54:28,510 - INFO - PCA applied: 30 components retained\n",
      "2025-09-06 23:54:28,512 - INFO - Final feature shape: (50, 30)\n",
      "2025-09-06 23:54:28,517 - INFO - Training set: 40 samples\n",
      "2025-09-06 23:54:28,518 - INFO - Test set: 10 samples\n",
      "2025-09-06 23:54:28,520 - INFO - Training models with cross-validation...\n",
      "2025-09-06 23:54:28,522 - INFO - Training Multinomial_NB...\n",
      "2025-09-06 23:54:28,565 - ERROR - Error training Multinomial_NB: \n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\Gen_AI\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\Gen_AI\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\Gen_AI\\Lib\\site-packages\\sklearn\\naive_bayes.py\", line 762, in fit\n",
      "    self._count(X, Y)\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\Gen_AI\\Lib\\site-packages\\sklearn\\naive_bayes.py\", line 889, in _count\n",
      "    check_non_negative(X, \"MultinomialNB (input X)\")\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\Gen_AI\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1824, in check_non_negative\n",
      "    raise ValueError(f\"Negative values in data passed to {whom}.\")\n",
      "ValueError: Negative values in data passed to MultinomialNB (input X).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Running Enhanced Fake News Detection System v2.0...\n",
      "============================================================\n",
      "ENHANCED FAKE NEWS DETECTION SYSTEM V2.0\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 23:54:28,567 - INFO - Training Logistic_Regression...\n",
      "2025-09-06 23:54:28,609 - INFO - Logistic_Regression - CV: 1.0000 (±0.0000)\n",
      "2025-09-06 23:54:28,611 - INFO - Training Random_Forest...\n",
      "2025-09-06 23:54:29,749 - INFO - Random_Forest - CV: 1.0000 (±0.0000)\n",
      "2025-09-06 23:54:29,751 - INFO - Training SVM...\n",
      "2025-09-06 23:54:29,797 - INFO - SVM - CV: 1.0000 (±0.0000)\n",
      "2025-09-06 23:54:29,798 - INFO - Training Gradient_Boosting...\n",
      "2025-09-06 23:54:30,192 - INFO - Gradient_Boosting - CV: 1.0000 (±0.0000)\n",
      "2025-09-06 23:54:30,193 - INFO - Ensemble model created successfully\n",
      "2025-09-06 23:54:30,195 - INFO - Comprehensive model evaluation...\n",
      "2025-09-06 23:54:30,208 - INFO - Logistic_Regression - Accuracy: 1.0000, F1: 1.0000, AUC: 1.0000\n",
      "2025-09-06 23:54:30,234 - INFO - Random_Forest - Accuracy: 1.0000, F1: 1.0000, AUC: 1.0000\n",
      "2025-09-06 23:54:30,251 - INFO - SVM - Accuracy: 1.0000, F1: 1.0000, AUC: 1.0000\n",
      "2025-09-06 23:54:30,265 - INFO - Gradient_Boosting - Accuracy: 1.0000, F1: 1.0000, AUC: 1.0000\n",
      "2025-09-06 23:54:30,266 - ERROR - Error evaluating ensemble: This VotingClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "2025-09-06 23:54:31,163 - INFO - Visualization report saved to reports/\n",
      "2025-09-06 23:54:31,164 - INFO - Preprocessing and extracting enhanced features...\n",
      "2025-09-06 23:54:31,168 - INFO - PCA applied: 30 components retained\n",
      "2025-09-06 23:54:31,169 - INFO - Final feature shape: (1, 30)\n",
      "2025-09-06 23:54:31,182 - ERROR - Error in ensemble prediction: This VotingClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "2025-09-06 23:54:31,184 - INFO - Preprocessing and extracting enhanced features...\n",
      "2025-09-06 23:54:31,191 - INFO - PCA applied: 30 components retained\n",
      "2025-09-06 23:54:31,193 - INFO - Final feature shape: (1, 30)\n",
      "2025-09-06 23:54:31,207 - ERROR - Error in ensemble prediction: This VotingClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "2025-09-06 23:54:31,210 - INFO - Preprocessing and extracting enhanced features...\n",
      "2025-09-06 23:54:31,215 - INFO - PCA applied: 30 components retained\n",
      "2025-09-06 23:54:31,216 - INFO - Final feature shape: (1, 30)\n",
      "2025-09-06 23:54:31,230 - ERROR - Error in ensemble prediction: This VotingClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "2025-09-06 23:54:31,232 - INFO - Preprocessing and extracting enhanced features...\n",
      "2025-09-06 23:54:31,240 - INFO - PCA applied: 30 components retained\n",
      "2025-09-06 23:54:31,241 - INFO - Final feature shape: (1, 30)\n",
      "2025-09-06 23:54:31,254 - ERROR - Error in ensemble prediction: This VotingClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "2025-09-06 23:54:31,256 - INFO - Preprocessing and extracting enhanced features...\n",
      "2025-09-06 23:54:31,261 - INFO - PCA applied: 30 components retained\n",
      "2025-09-06 23:54:31,262 - INFO - Final feature shape: (1, 30)\n",
      "2025-09-06 23:54:31,275 - ERROR - Error in ensemble prediction: This VotingClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "2025-09-06 23:54:31,288 - INFO - Enhanced model saved to enhanced_fake_news_detector_v2.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TESTING WITH ENHANCED PREDICTIONS\n",
      "============================================================\n",
      "\n",
      "Test Article 1: Scientists at MIT have developed a revolutionary new battery technology accordin...\n",
      "\n",
      "Predictions:\n",
      "  Logistic_Regression: Real (Confidence: 0.992)\n",
      "  Random_Forest: Real (Confidence: 0.710)\n",
      "  SVM: Real (Confidence: 0.984)\n",
      "  Gradient_Boosting: Real (Confidence: 1.000)\n",
      "  Consensus: Real (Confidence: 0.921)\n",
      "\n",
      "Explanation: This article is classified as REAL news with 92.1% confidence.\n",
      "Confidence Level: Very High\n",
      "Recommendation: Appears to be legitimate news, but still verify if sharing important information.\n",
      "\n",
      "Test Article 2: BREAKING: Government officials confirm alien contact, world leaders to make SHOC...\n",
      "\n",
      "Predictions:\n",
      "  Logistic_Regression: Fake (Confidence: 0.990)\n",
      "  Random_Forest: Fake (Confidence: 0.760)\n",
      "  SVM: Fake (Confidence: 0.980)\n",
      "  Gradient_Boosting: Fake (Confidence: 1.000)\n",
      "  Consensus: Fake (Confidence: 0.933)\n",
      "\n",
      "Explanation: This article is classified as FAKE news with 93.3% confidence.\n",
      "Confidence Level: Very High\n",
      "Key Indicators:\n",
      "  - Contains high emotional/sensational language\n",
      "  - Contains excessive exclamation marks\n",
      "Recommendation: Exercise extreme caution. Verify through multiple reliable sources before sharing.\n",
      "\n",
      "Test Article 3: Local hospital reports successful treatment outcomes with new COVID-19 therapy i...\n",
      "\n",
      "Predictions:\n",
      "  Logistic_Regression: Real (Confidence: 0.960)\n",
      "  Random_Forest: Real (Confidence: 0.880)\n",
      "  SVM: Real (Confidence: 0.990)\n",
      "  Gradient_Boosting: Real (Confidence: 1.000)\n",
      "  Consensus: Real (Confidence: 0.958)\n",
      "\n",
      "Explanation: This article is classified as REAL news with 95.8% confidence.\n",
      "Confidence Level: Very High\n",
      "Recommendation: Appears to be legitimate news, but still verify if sharing important information.\n",
      "\n",
      "Test Article 4: DOCTORS DON'T WANT YOU TO KNOW this ancient herbal remedy cures everything insta...\n",
      "\n",
      "Predictions:\n",
      "  Logistic_Regression: Fake (Confidence: 0.936)\n",
      "  Random_Forest: Fake (Confidence: 0.820)\n",
      "  SVM: Fake (Confidence: 0.980)\n",
      "  Gradient_Boosting: Fake (Confidence: 1.000)\n",
      "  Consensus: Fake (Confidence: 0.934)\n",
      "\n",
      "Explanation: This article is classified as FAKE news with 93.4% confidence.\n",
      "Confidence Level: Very High\n",
      "Recommendation: Exercise extreme caution. Verify through multiple reliable sources before sharing.\n",
      "\n",
      "Test Article 5: Federal Reserve announces interest rate changes following comprehensive economic...\n",
      "\n",
      "Predictions:\n",
      "  Logistic_Regression: Real (Confidence: 0.976)\n",
      "  Random_Forest: Real (Confidence: 0.820)\n",
      "  SVM: Real (Confidence: 0.992)\n",
      "  Gradient_Boosting: Real (Confidence: 1.000)\n",
      "  Consensus: Real (Confidence: 0.947)\n",
      "\n",
      "Explanation: This article is classified as REAL news with 94.7% confidence.\n",
      "Confidence Level: Very High\n",
      "Recommendation: Appears to be legitimate news, but still verify if sharing important information.\n",
      "\n",
      "============================================================\n",
      "ENHANCED SYSTEM TRAINING AND TESTING COMPLETE!\n",
      "============================================================\n",
      "\n",
      "✅ System initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "from enhanced_fake_news_detector_v2 import main_enhanced\n",
    "\n",
    "print(\"🚀 Running Enhanced Fake News Detection System v2.0...\")\n",
    "detector = main_enhanced()\n",
    "\n",
    "if detector:\n",
    "    print(\"\\n✅ System initialized successfully!\")\n",
    "else:\n",
    "    print(\"❌ System failed to initialize\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded76c37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
